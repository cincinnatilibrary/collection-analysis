{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!!!\n",
    "# make sure that you run the backup first!\n",
    "# /home/plchuser/.backup/plch-ilsaux2-collection-analysis.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note \n",
    "make sure to run the backup script first ...\n",
    "\n",
    "`/home/plchuser/.backup/plch-ilsaux2-collection-analysis.sh`\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# backup for collection analysis files for local and collectionHQ\n",
    "\n",
    "unset HISTFILE\n",
    "export B2_ACCOUNT_ID=\"\"\n",
    "export B2_ACCOUNT_KEY=\"\"\n",
    "export RESTIC_REPOSITORY=\"b2:plch-collection-analysis\"\n",
    "export RESTIC_PASSWORD=\"\"\n",
    "export RESTIC_CACHE_DIR=\"/home/plchuser/.cache/restic\"\n",
    "\n",
    "# we should only need to do this on an empty repo\n",
    "# /home/plchuser/.backup/restic/restic init\n",
    "\n",
    "cd /home/plchuser/output/collection-analysis\n",
    "/usr/bin/find *.csv -print0 | /usr/bin/xargs -0 xz -9 -T0\n",
    "\n",
    "# add --verbose after restic command for more info\n",
    "/home/plchuser/.backup/restic/restic \\\n",
    "        backup \\\n",
    "        /home/plchuser/output/collection-analysis\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n",
      "/home/plchuser/output/jupyter/collection-analysis/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.7/site-packages (21.3.1)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.7/site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./venv/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./venv/lib/python3.7/site-packages (from pandas) (1.21.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sqlalchemy in ./venv/lib/python3.7/site-packages (1.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./venv/lib/python3.7/site-packages (from sqlalchemy) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata in ./venv/lib/python3.7/site-packages (from sqlalchemy) (3.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./venv/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy) (3.10.0.0)\n",
      "Requirement already satisfied: psycopg2-binary in ./venv/lib/python3.7/site-packages (2.9.1)\n",
      "Requirement already satisfied: sqlite-utils in ./venv/lib/python3.7/site-packages (3.17.1)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.7/site-packages (from sqlite-utils) (8.0.1)\n",
      "Requirement already satisfied: tabulate in ./venv/lib/python3.7/site-packages (from sqlite-utils) (0.8.9)\n",
      "Requirement already satisfied: sqlite-fts4 in ./venv/lib/python3.7/site-packages (from sqlite-utils) (1.0.1)\n",
      "Requirement already satisfied: click-default-group in ./venv/lib/python3.7/site-packages (from sqlite-utils) (1.2.2)\n",
      "Requirement already satisfied: dateutils in ./venv/lib/python3.7/site-packages (from sqlite-utils) (0.6.12)\n",
      "Requirement already satisfied: importlib-metadata in ./venv/lib/python3.7/site-packages (from click->sqlite-utils) (3.10.1)\n",
      "Requirement already satisfied: pytz in ./venv/lib/python3.7/site-packages (from dateutils->sqlite-utils) (2021.1)\n",
      "Requirement already satisfied: python-dateutil in ./venv/lib/python3.7/site-packages (from dateutils->sqlite-utils) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata->click->sqlite-utils) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./venv/lib/python3.7/site-packages (from importlib-metadata->click->sqlite-utils) (3.10.0.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil->dateutils->sqlite-utils) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U pandas\n",
    "!pip install -U sqlalchemy\n",
    "!pip install -U psycopg2-binary\n",
    "!pip install -U sqlite-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Integer, BigInteger, Numeric\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from decimal import Decimal\n",
    "import sqlite_utils\n",
    "\n",
    "import vars\n",
    "\n",
    "engine = create_engine('sqlite:///current_collection.db', echo=False)\n",
    "\n",
    "sierra_engine = create_engine('postgresql://{}:{}@sierra-db.plch.net:1032/iii'.format(vars.pg_username, vars.pg_password))\n",
    "\n",
    "collection_file_path = os.path.join(os.getcwd(), '/home/plchuser/output/collection-analysis/')\n",
    "\n",
    "item_re = re.compile(r\"^[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}\\-plch\\-item\\.csv\\.xz\")\n",
    "bib_re = re.compile(r\"^[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}\\-plch\\-bib\\.csv\\.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n"
     ]
    }
   ],
   "source": [
    "# this is our working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE and refresh the sqlite database file in the local directory\n",
    "try:\n",
    "    os.remove('current_collection.db')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.close(os.open('current_collection.db', os.O_CREAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_files = os.listdir(collection_file_path)\n",
    "collection_files.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert price to integer values\n",
    "numbers_only_re = re.compile('[^0-9]')\n",
    "\n",
    "def price_to_int(price):\n",
    "    return int(numbers_only_re.sub('', price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-01-plch-item.csv.xz\n"
     ]
    }
   ],
   "source": [
    "item_file = [file for file in collection_files if item_re.match(file)][0]\n",
    "print(item_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was here to generate the database for the first snapshot of the year\n",
    "# item_file = '2020-01-06-plch-item.csv.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(collection_file_path, item_file),\n",
    "    compression='xz',\n",
    "    delimiter='|',\n",
    "    converters={'price': price_to_int},\n",
    "    # nrows=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'price': 'price_cents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name='item', index=False, if_exists='replace', con=engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-01-plch-bib.csv.xz\n"
     ]
    }
   ],
   "source": [
    "bib_file = [file for file in collection_files if bib_re.match(file)][0]\n",
    "print(bib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was here to generate the database for the first snapshot of the year\n",
    "# bib_file = '2020-01-06-plch-bib.csv.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(collection_file_path, bib_file),\n",
    "    compression='xz',\n",
    "    delimiter='|',\n",
    ")\n",
    "\n",
    "df.to_sql(\n",
    "    name='bib', \n",
    "    index=False, \n",
    "    if_exists='replace', \n",
    "    con=engine, \n",
    "    chunksize=10000,\n",
    "    dtype={\n",
    "        'publish_year': Integer(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes to create\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX IF NOT EXISTS \"idx_bib_bib_record_num\" ON \"bib\" (\n",
    "    \"bib_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_bib_indexed_subjects\" on bib (\n",
    "    \"indexed_subjects\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_format_item_status_location_code_item_callnumber\" ON \"item\" (\n",
    "    \"location_code\",\n",
    "    \"item_format\",\n",
    "    \"item_status_code\",\n",
    "    \"item_callnumber\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_bib_record_num\" ON \"item\" (\n",
    "    \"bib_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_record_num\" ON \"item\" (\n",
    "    \"item_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_agency_code_num_location_code\" ON \"item\" (\n",
    "    \"agency_code_num\",\n",
    "    \"location_code\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_barcode\" ON \"item\" (\n",
    "    \"barcode\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_format\" ON \"item\" (\n",
    "    \"item_format\"\n",
    ");\n",
    "\"\"\"\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000, 1000000 1000000, 2000000 95369, 3000000 0, "
     ]
    }
   ],
   "source": [
    "# bib_record\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "b.id,\n",
    "b.record_id,\n",
    "b.language_code,\n",
    "b.bcode1,\n",
    "b.bcode2,\n",
    "b.bcode3,\n",
    "b.country_code,\n",
    "b.index_change_count,\n",
    "b.is_on_course_reserve,\n",
    "b.is_right_result_exact,\n",
    "b.allocation_rule_code,\n",
    "b.skip_num,\n",
    "b.cataloging_date_gmt,\n",
    "b.marc_type_code,\n",
    "b.is_suppressed\n",
    "\n",
    "FROM\n",
    "sierra_view.bib_record as b\n",
    "\n",
    "JOIN\n",
    "sierra_view.record_metadata as r on r.id = b.record_id\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "\n",
    "ORDER BY\n",
    "b.id\n",
    "\n",
    "LIMIT 1000000 OFFSET {}\n",
    "\"\"\"\n",
    "\n",
    "# start the offset at 0, then add 100000 to the offset\n",
    "offset = 0\n",
    "count = 0\n",
    "while (True):\n",
    "    df = pd.read_sql(sql=sql.format(offset), con=sierra_engine)        \n",
    "    print(offset, df.shape[0], sep=' ', end=', ')\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    df.to_sql(\n",
    "        name='bib_record',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'record_id': Integer(),\n",
    "            'index_change_count': Integer(),\n",
    "            'skip_num': Integer(),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    count += 1\n",
    "    offset += 1000000\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX bib_record_bcode3_idx ON bib_record (bcode3);\n",
    "CREATE INDEX bib_record_bib_level_idx ON bib_record (bcode1);\n",
    "CREATE INDEX bib_record_country_idx ON bib_record (country_code);\n",
    "CREATE INDEX bib_record_lang_idx ON bib_record (language_code);\n",
    "CREATE INDEX bib_record_material_type_idx ON bib_record (bcode2);\n",
    "CREATE UNIQUE INDEX bib_record_record_key ON bib_record (record_id);\n",
    "CREATE INDEX idx_bib_record_cataloging_date ON bib_record (cataloging_date_gmt);\n",
    "CREATE UNIQUE INDEX pk_bib_record ON bib_record (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language_property\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "p.id,\n",
    "p.code,\n",
    "p.display_order,\n",
    "n.name\n",
    "\n",
    "from\n",
    "sierra_view.language_property as p\n",
    "\n",
    "join\n",
    "sierra_view.language_property_name as n\n",
    "on n.language_property_id = p.id\n",
    "\n",
    "order by id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "df.to_sql(\n",
    "        name='language_property',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'display_order': Integer(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX language_property_code_key ON language_property (code);\n",
    "CREATE UNIQUE INDEX pk_language_property ON language_property (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sierra_view.record_metadata\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "r.id,\n",
    "r.record_type_code,\n",
    "r.record_num,\n",
    "date(r.creation_date_gmt) as creation_date_gmt,\n",
    "date(r.deletion_date_gmt) as deletion_date_gmt,\n",
    "r.campus_code,\n",
    "r.agency_code_num,\n",
    "r.record_last_updated_gmt\n",
    "\n",
    "FROM\n",
    "sierra_view.record_metadata as r\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "-- started grabbing the deleted record data 2021-03-15\n",
    "AND r.deletion_date_gmt IS NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j' ) -- bibliographic, item, volume\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine )\n",
    "\n",
    "# output to the sqlite db\n",
    "# NOTE: the first time through, we'll want to \"replace\" ... the second \"append\"\n",
    "df.to_sql(name='record_metadata', index=False, if_exists='replace', con=engine, chunksize=10000)\n",
    "\n",
    "# -- started grabbing the deleted record data 2021-03-15\n",
    "# doing this as the second part since it may exceeed our memory limits\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "r.id,\n",
    "r.record_type_code,\n",
    "r.record_num,\n",
    "date(r.creation_date_gmt) as creation_date_gmt,\n",
    "date(r.deletion_date_gmt) as deletion_date_gmt,\n",
    "r.campus_code,\n",
    "r.agency_code_num,\n",
    "r.record_last_updated_gmt\n",
    "\n",
    "FROM\n",
    "sierra_view.record_metadata as r\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "-- started grabbing the deleted record data 2021-03-15\n",
    "AND r.deletion_date_gmt IS NOT NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j' ) -- bibliographic, item, volume\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine )\n",
    "\n",
    "# output to the sqlite db\n",
    "df.to_sql(name='record_metadata', index=False, if_exists='append', con=engine, chunksize=10000)\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_record_metadata_id_record_last_updated ON record_metadata (id, record_last_updated_gmt);\n",
    "CREATE INDEX idx_record_metadata_record_creation_date_gmt ON record_metadata (creation_date_gmt);\n",
    "CREATE INDEX idx_record_metadata_record_num ON record_metadata (record_num);\n",
    "CREATE INDEX pk_record_id ON record_metadata (id);\n",
    "CREATE INDEX record_id_unique_constraint ON record_metadata (record_type_code, record_num, campus_code);\n",
    "CREATE INDEX record_metadata_last_modified ON record_metadata (record_last_updated_gmt, record_type_code, id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6212726\n",
      "         id  bib_record_id  bib_record_num  item_record_id  item_record_num  \\\n",
      "0  12300587   420910445515         3650507    450982845989         11279909   \n",
      "1   4795612   420909381400         2586392    450975891312          4325232   \n",
      "2   6881692   420909328850         2533842    450977977395          6411315   \n",
      "3   4075845   420908782499         1987491    450975171545          3605465   \n",
      "4   4686952   420909047964         2252956    450975782654          4216574   \n",
      "\n",
      "   items_display_order  bibs_display_order  \n",
      "0                  0.0                   0  \n",
      "1                  1.0                   0  \n",
      "2                 16.0                   0  \n",
      "3                  2.0                   0  \n",
      "4                  1.0                   0  \n"
     ]
    }
   ],
   "source": [
    "# bib_record_item_record_link\n",
    "sql = \"\"\"\n",
    "select \n",
    "l.id,\n",
    "l.bib_record_id,\n",
    "r.record_num as bib_record_num,\n",
    "l.item_record_id,\n",
    "ir.record_num as item_record_num,\n",
    "l.items_display_order,\n",
    "l.bibs_display_order\n",
    "\n",
    "from\n",
    "sierra_view.bib_record_item_record_link as l\n",
    "\n",
    "join sierra_view.record_metadata as r on r.id = l.bib_record_id\n",
    "\n",
    "join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'bib_record_item_record_link', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'bib_record_id': BigInteger(),\n",
    "        'bib_record_num': Integer(), \n",
    "        'item_record_id': BigInteger(),\n",
    "        'item_record_num': Integer(),\n",
    "        'items_display_order': Integer(),\n",
    "        'bibs_display_order': Integer(), \n",
    "    }\n",
    ")\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_bib_record_item_record_link_bib_record_id ON bib_record_item_record_link (bib_record_id);\n",
    "CREATE INDEX idx_bib_record_item_record_link_bib_record_num ON bib_record_item_record_link (bib_record_num);\n",
    "CREATE INDEX item_record_id_index ON bib_record_item_record_link (item_record_id);\n",
    "CREATE INDEX item_record_num_index ON bib_record_item_record_link (item_record_num);\n",
    "CREATE UNIQUE INDEX pk_bib_record_item_record_link ON bib_record_item_record_link (id);\n",
    "CREATE UNIQUE INDEX uc_bib_record_item_record_link ON bib_record_item_record_link (bib_record_id, item_record_id);\n",
    "CREATE INDEX ucn_bib_record_item_record_link ON bib_record_item_record_link (bib_record_num, item_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701109\n",
      "        id  volume_record_id  volume_record_num  item_record_id  \\\n",
      "0   667378      455267901036            1367660    450977865780   \n",
      "1  1090535      455267965493            1432117    450979552158   \n",
      "2  1797671      455268051688            1518312    450982824418   \n",
      "3  1411925      455268006622            1473246    450980991384   \n",
      "4  1676652      455268039201            1505825    450982260146   \n",
      "\n",
      "   item_record_num  items_display_order      volume_statement  \n",
      "0          6299700                  NaN                  v.03  \n",
      "1          7986078                  NaN               v.25/26  \n",
      "2         11258338                  0.0  NO. 172 Sep/Oct 2021  \n",
      "3          9425304                  NaN  V. 82 NO. 7 Jul 2017  \n",
      "4         10694066                  6.0  V. 41 NO. 1 Jan 2020  \n"
     ]
    }
   ],
   "source": [
    "# volume_record_item_record_link\n",
    "sql = \"\"\"\n",
    "select\n",
    "l.id,\n",
    "l.volume_record_id,\n",
    "vr.record_num as volume_record_num,\n",
    "l.item_record_id,\n",
    "ir.record_num as item_record_num,\n",
    "l.items_display_order,\n",
    "(\n",
    "    select\n",
    "    string_agg(v.field_content, ', ' order by occ_num)\n",
    "\n",
    "    from\n",
    "    sierra_view.varfield as v\n",
    "\n",
    "    where\n",
    "    v.record_id = l.volume_record_id\n",
    "    and v.varfield_type_code = 'v'\n",
    ") as volume_statement\n",
    "\n",
    "from\n",
    "sierra_view.volume_record_item_record_link as l\n",
    "join sierra_view.record_metadata as vr on vr.id = l.volume_record_id\n",
    "join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'volume_record_item_record_link', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'volume_record_id': BigInteger(),\n",
    "        'volume_record_num': Integer(), \n",
    "        'item_record_id': BigInteger(),\n",
    "        'item_record_num': Integer(),\n",
    "        'items_display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_volume_record_item_record_link_volume_record_id ON volume_record_item_record_link (volume_record_id);\n",
    "CREATE INDEX idx_volume_record_item_record_link_volume_record_num ON volume_record_item_record_link (volume_record_num);\n",
    "CREATE UNIQUE INDEX pk_volume_record_item_record_link ON volume_record_item_record_link (id);\n",
    "CREATE UNIQUE INDEX uc_volume_record_item_record_link ON volume_record_item_record_link (item_record_id);\n",
    "CREATE INDEX volume_record_item_record_link_item_id_volume_id ON volume_record_item_record_link (item_record_id, volume_record_id);\n",
    "CREATE INDEX volume_record_item_record_link_item_num_volume_num ON volume_record_item_record_link (item_record_num, volume_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000, 1000000 1000000, 2000000 1000000, 3000000 1000000, 4000000 1000000, 5000000 1000000, 6000000 1000000, 7000000 398358, 8000000 0, "
     ]
    }
   ],
   "source": [
    "# phrase_entry\n",
    "\n",
    "# TODO\n",
    "# we're going to skip this for now, since there doesn't seem to be a big benefit to this\n",
    "\n",
    "# target these index_tag values:\n",
    "# \"d\": \"subject\"\n",
    "# \"a\": \"author\",\n",
    "# \"t\": \"title\",\n",
    "# \"o\": \"ocolc\",\n",
    "# \"c\": \"callnumber\",\n",
    "# \"i\": \"isbn\",\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT \n",
    "e.id,\n",
    "e.record_id,\n",
    "e.index_tag,\n",
    "e.varfield_type_code,\n",
    "e.occurrence,\n",
    "e.is_permuted,\n",
    "e.type2,\n",
    "e.type3,\n",
    "e.index_entry,\n",
    "e.insert_title,\n",
    "e.phrase_rule_rule_num,\n",
    "e.phrase_rule_operation,\n",
    "e.phrase_rule_subfield_list,\n",
    "e.original_content,\n",
    "e.parent_record_id,\n",
    "e.insert_title_tag,\n",
    "e.insert_title_occ\n",
    "\n",
    "FROM sierra_view.phrase_entry as e\n",
    "\n",
    "JOIN\n",
    "sierra_view.record_metadata as r\n",
    "ON\n",
    "  r.id = e.record_id\n",
    "  \n",
    "WHERE\n",
    "e.index_tag in (\n",
    "    'd'\n",
    "    -- add these back later maybe\n",
    "    -- , 'a', 't', 'o', 'c', 'i'\n",
    ")\n",
    "AND r.campus_code = ''\n",
    "AND r.deletion_date_gmt IS NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j') -- bibliographic, item, volume\n",
    "\n",
    "ORDER BY\n",
    "id\n",
    "\n",
    "LIMIT 1000000 OFFSET {}\n",
    "\"\"\"\n",
    "\n",
    "# start the offset at 0, then add 100000 to the offset\n",
    "offset = 0\n",
    "count = 0\n",
    "while (True):\n",
    "    df = pd.read_sql(sql=sql.format(offset), con=sierra_engine)        \n",
    "    print(offset, df.shape[0], sep=' ', end=', ')\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    df.to_sql(\n",
    "        name='phrase_entry',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'record_id': Integer(),\n",
    "            'occurrence': Integer(),\n",
    "            'type2': Integer(),\n",
    "            'phrase_rule_rule_num': Integer(),\n",
    "            'parent_record_id': Integer(),\n",
    "            'insert_title_occ': Integer(),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    count += 1\n",
    "    offset += 1000000\n",
    "    \n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_phrase_entry ON phrase_entry (((index_tag || index_entry)), type2, insert_title, record_id);\n",
    "CREATE INDEX idx_phrase_entry_parent_record_id ON phrase_entry (parent_record_id);\n",
    "CREATE INDEX idx_phrase_entry_record ON phrase_entry (record_id);\n",
    "CREATE INDEX idx_phrase_entry_record_key ON phrase_entry (record_id);\n",
    "CREATE UNIQUE INDEX pk_phrase_entry ON phrase_entry (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009\n",
      "     id   code  branch_code_num parent_location_code  is_public  \\\n",
      "0   884  fotab             14.0                 None      False   \n",
      "1    81  y0604              1.0                 None      False   \n",
      "2  1683  nsjdn             29.0                 None      False   \n",
      "3    27  avjpl              3.0                 None      False   \n",
      "4   616  crzzz              9.0                 None      False   \n",
      "\n",
      "   is_requestable  \n",
      "0            True  \n",
      "1            True  \n",
      "2            True  \n",
      "3            True  \n",
      "4            True  \n"
     ]
    }
   ],
   "source": [
    "# location\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "code,\n",
    "branch_code_num,\n",
    "parent_location_code,\n",
    "is_public,\n",
    "is_requestable\n",
    "FROM\n",
    "sierra_view.location;\n",
    ";\n",
    "\"\"\"\n",
    "    \n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code': Integer(),\n",
    "        'branch_code_num': Integer(),\n",
    "        'parent_location_code': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk9ff58fb55804fddb ON location (branch_code_num);\n",
    "CREATE UNIQUE INDEX location_code_key ON location (code);\n",
    "CREATE UNIQUE INDEX pk_location ON location (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "   branch_id          name  iii_language_id\n",
      "0          1  Main Library                1\n",
      "1          2      Anderson                1\n",
      "2          3      Avondale                1\n",
      "3          4      Blue Ash                1\n",
      "4          5     Bond Hill                1\n"
     ]
    }
   ],
   "source": [
    "# branch_name\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "branch_id,\n",
    "name,\n",
    "iii_language_id\n",
    "FROM\n",
    "sierra_view.branch_name;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'branch_name', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'branch_id': Integer(),\n",
    "        'iii_language_id': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX branch_name_pkey ON branch_name (branch_id, iii_language_id);\n",
    "CREATE INDEX fk46f5c7085804fddb ON branch_name (branch_id);\n",
    "CREATE INDEX fk46f5c7088eaffe82 ON branch_name (iii_language_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "   id                                            address  \\\n",
      "0  52                                                      \n",
      "1  50  Main Library$800 Vine Street$Cincinnati, Ohio ...   \n",
      "2  17  Greenhills Branch$8 Enfield St.$Cincinnati, Oh...   \n",
      "3  49                                                      \n",
      "4  44                                                      \n",
      "\n",
      "                          email_source                       email_reply_to  \\\n",
      "0                                                                             \n",
      "1  patronnotices@cincinnatilibrary.org  patronnotices@cincinnatilibrary.org   \n",
      "2  patronnotices@cincinnatilibrary.org  patronnotices@cincinnatilibrary.org   \n",
      "3                                                                             \n",
      "4  patronnotices@cincinnatilibrary.org  patronnotices@cincinnatilibrary.org   \n",
      "\n",
      "  address_latitude address_longitude  code_num  \n",
      "0       39.1057790       -84.5133140        52  \n",
      "1       39.2305206       -84.3749388        50  \n",
      "2       39.2682526       -84.5221859        16  \n",
      "3             None              None        49  \n",
      "4             None              None        46  \n"
     ]
    }
   ],
   "source": [
    "# branch\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "address,\n",
    "email_source,\n",
    "email_reply_to,\n",
    "address_latitude,\n",
    "address_longitude,\n",
    "code_num\n",
    "\n",
    "FROM \n",
    "sierra_view.branch;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'branch', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code_num': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX pk_branch ON branch (id);\n",
    "CREATE UNIQUE INDEX uniq_branch_code_num ON branch (code_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "  code  display_order                      name\n",
      "0                   0                No country\n",
      "1   aa              1                   Albania\n",
      "2  abc              2                   Alberta\n",
      "3  aca              3  Australian Capital Terr.\n",
      "4   ae              4                   Algeria\n"
     ]
    }
   ],
   "source": [
    "# country_property_myuser\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    "from\n",
    "\n",
    "sierra_view.country_property_myuser\n",
    "\n",
    "order by code\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'country_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "  code  display_order                       name\n",
      "0    !              0               ON HOLDSHELF\n",
      "1    #              1       SearchOH/OL RECEIVED\n",
      "2    $              2              LOST AND PAID\n",
      "3    %              3       SearchOH/OL RETURNED\n",
      "4    &              4  SearchOH/OHIOLINK REQUEST\n"
     ]
    }
   ],
   "source": [
    "# item_status_property_myuser\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.item_status_property_myuser\n",
    "order by display_order\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'item_status_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "   code  display_order itype_property_category_id  physical_format_id  \\\n",
      "0   105             54                       None                 NaN   \n",
      "1     6              6                       None                 NaN   \n",
      "2   102             51                       None                 NaN   \n",
      "3   110             55                       None                 NaN   \n",
      "4     2              2                       None                 1.0   \n",
      "\n",
      "  target_audience_id                  name  \n",
      "0               None            Leased DVD  \n",
      "1               None           Leased Book  \n",
      "2               None                Bluray  \n",
      "3               None  MakerSpace Equipment  \n",
      "4               None         Juvenile Book  \n"
     ]
    }
   ],
   "source": [
    "# itype_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "code,\n",
    "display_order,\n",
    "itype_property_category_id,\n",
    "physical_format_id,\n",
    "target_audience_id,\n",
    "name\n",
    "\n",
    "FROM \n",
    "sierra_view.itype_property_myuser\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'itype_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'code': Integer(),\n",
    "        'display_order': Integer(),\n",
    "        'physical_format_id': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "   id  is_default  display_order         name\n",
      "0   1        True              1         Book\n",
      "1  29       False              2   Book on CD\n",
      "2  34       False              3          DVD\n",
      "3  35       False              4  Large Print\n",
      "4  33       False              5     Magazine\n"
     ]
    }
   ],
   "source": [
    "# physical_format_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.physical_format_myuser\n",
    "order by display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'physical_format_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "  code  display_order          name\n",
      "0    -              0           ---\n",
      "1    a              1  MONO COMP PT\n",
      "2    b              2   SER COMP PT\n",
      "3    c              3    COLLECTION\n",
      "4    d              4       SUBUNIT\n"
     ]
    }
   ],
   "source": [
    "# bib_level_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.bib_level_property_myuser\n",
    "\n",
    "order by\n",
    "display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'bib_level_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "  code  display_order  is_public material_property_category_id  \\\n",
      "0    -              0       True                          None   \n",
      "1    1              1       True                          None   \n",
      "2    2              2       True                          None   \n",
      "3    3              3       True                          None   \n",
      "4    4              4       True                          None   \n",
      "\n",
      "  physical_format_id                    name  \n",
      "0               None               Undefined  \n",
      "1               None  Downloadable Audiobook  \n",
      "2               None       Downloadable Book  \n",
      "3               None      Downloadable Music  \n",
      "4               None      Downloadable Video  \n"
     ]
    }
   ],
   "source": [
    "# material_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.material_property_myuser\n",
    "\n",
    "order by\n",
    "display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'material_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009\n",
      "     id   code  branch_code_num parent_location_code  is_public  \\\n",
      "0   884  fotab             14.0                 None      False   \n",
      "1    81  y0604              1.0                 None      False   \n",
      "2  1683  nsjdn             29.0                 None      False   \n",
      "3    27  avjpl              3.0                 None      False   \n",
      "4   616  crzzz              9.0                 None      False   \n",
      "\n",
      "   is_requestable  \n",
      "0            True  \n",
      "1            True  \n",
      "2            True  \n",
      "3            True  \n",
      "4            True  \n"
     ]
    }
   ],
   "source": [
    "# location\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "code,\n",
    "branch_code_num,\n",
    "parent_location_code,\n",
    "is_public,\n",
    "is_requestable\n",
    "FROM\n",
    "sierra_view.location;\n",
    ";\n",
    "\"\"\"\n",
    "    \n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code': Integer(),\n",
    "        'branch_code_num': Integer(),\n",
    "        'parent_location_code': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk9ff58fb55804fddb ON location (branch_code_num);\n",
    "CREATE UNIQUE INDEX location_code_key ON location (code);\n",
    "CREATE UNIQUE INDEX pk_location ON location (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009\n",
      "   location_id                                 name  iii_language_id\n",
      "0          884          Forest Park Teen Audiobooks                1\n",
      "1         2177         Walnut Hills Television DVDs                1\n",
      "2         1683  Northside Juvenile New Release DVDs                1\n",
      "3         2338                Wyoming Foreign Films                1\n",
      "4          616                   Corryville Cleanup                1\n"
     ]
    }
   ],
   "source": [
    "# location_name\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "location_id,\n",
    "name,\n",
    "iii_language_id\n",
    "FROM sierra_view.location_name;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location_name', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'location_id': Integer(),\n",
    "        'iii_language_id': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk506824d5399f0cbb ON location_name (location_id);\n",
    "CREATE INDEX fk506824d58eaffe82 ON location_name (iii_language_id);\n",
    "CREATE UNIQUE INDEX location_name_pkey ON location_name (location_id, iii_language_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156038\n",
      "   hold_id  bib_record_num campus_code record_type_on_hold  item_record_num  \\\n",
      "0  2707601         2621589                               j              NaN   \n",
      "1  2707604         2621589                               j              NaN   \n",
      "2  3473847         2621589                               j              NaN   \n",
      "3  3473848         2621589                               j              NaN   \n",
      "4  3473856         2823181                               j              NaN   \n",
      "\n",
      "   volume_record_num                 placed_gmt  is_frozen  delay_days  \\\n",
      "0          1386395.0  2013-01-21 16:11:15-05:00       True         255   \n",
      "1          1366848.0  2013-01-21 16:11:23-05:00       True         255   \n",
      "2          1386395.0  2013-03-11 19:09:43-04:00       True         255   \n",
      "3          1366848.0  2013-03-11 19:09:51-04:00       True         255   \n",
      "4          1401384.0  2013-03-11 19:10:11-04:00       True         255   \n",
      "\n",
      "  location_code  ... pickup_location_code ir_pickup_location_code  \\\n",
      "0          None  ...                   hp                    None   \n",
      "1          None  ...                   hp                    None   \n",
      "2          None  ...                   ch                    None   \n",
      "3          None  ...                   ch                    None   \n",
      "4          None  ...                   ch                    None   \n",
      "\n",
      "   ir_print_name  ir_delivery_stop_name is_ir_converted_request  \\\n",
      "0           None                   None                   False   \n",
      "1           None                   None                   False   \n",
      "2           None                   None                   False   \n",
      "3           None                   None                   False   \n",
      "4           None                   None                   False   \n",
      "\n",
      "  patron_is_active patron_ptype_code patron_home_library_code  \\\n",
      "0             True                 0                       hp   \n",
      "1             True                 0                       hp   \n",
      "2             True                 0                       pr   \n",
      "3             True                 0                       pr   \n",
      "4             True                 0                       pr   \n",
      "\n",
      "   patron_mblock_code  patron_has_over_10usd_owed  \n",
      "0                   -                       False  \n",
      "1                   -                       False  \n",
      "2                   -                       False  \n",
      "3                   -                       False  \n",
      "4                   -                       False  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# hold\n",
    "\n",
    "sql = \"\"\"\\\n",
    "-- pull all relevant hold data\n",
    "\n",
    "select\n",
    "h.id as hold_id,\n",
    "case\n",
    "    when r.record_type_code = 'i' then (\n",
    "        select\n",
    "        br.record_num\n",
    "        from\n",
    "        sierra_view.bib_record_item_record_link as l\n",
    "        join sierra_view.record_metadata as br on br.id = l.bib_record_id\n",
    "        where\n",
    "        l.item_record_id = h.record_id\n",
    "        limit 1\n",
    "    )\n",
    "    when r.record_type_code = 'j' then (\n",
    "        select\n",
    "        br.record_num\n",
    "        from\n",
    "        sierra_view.bib_record_volume_record_link as l\n",
    "        join sierra_view.record_metadata as br on br.id = l.bib_record_id\n",
    "        where\n",
    "        l.volume_record_id = h.record_id\n",
    "        limit 1\n",
    "    )\n",
    "    when r.record_type_code = 'b' then r.record_num\n",
    "    else NULL\n",
    "end as bib_record_num,\n",
    "r.campus_code,\n",
    "r.record_type_code as record_type_on_hold,\n",
    "case\n",
    "    when r.record_type_code = 'i' then r.record_num\n",
    "-- i don't think this is really useful, but i may want to come back to this \n",
    "-- when r.record_type_code = 'j' then (\n",
    "-- select\n",
    "-- ir.record_num\n",
    "-- from\n",
    "-- sierra_view.volume_record_item_record_link as l\n",
    "-- join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "-- where\n",
    "-- l.volume_record_id = h.record_id\n",
    "-- limit 1\n",
    "-- )\n",
    "    else NULL\n",
    "end as item_record_num,\n",
    "case\n",
    "    when r.record_type_code = 'j' then r.record_num\n",
    "    else NULL\n",
    "end as volume_record_num,\n",
    "h.placed_gmt,\n",
    "h.is_frozen,\n",
    "h.delay_days,\n",
    "h.location_code,\n",
    "h.expires_gmt,\n",
    "case\n",
    "when h.status = '0' then 'on hold'\n",
    "when h.status = 'b' then 'bib hold ready for pickup'\n",
    "when h.status = 'j' then 'volume hold ready for pickup'\n",
    "when h.status = 'i' then 'item hold ready for pickup'\n",
    "when h.status = 't' then 'in transit to pickup location'\n",
    "else h.status\n",
    "end as hold_status,\n",
    "h.is_ir,\n",
    "h.is_ill,\n",
    "h.pickup_location_code,\n",
    "h.ir_pickup_location_code,\n",
    "h.ir_print_name,\n",
    "h.ir_delivery_stop_name,\n",
    "h.is_ir_converted_request,\n",
    "case\n",
    "when p.activity_gmt >= (NOW() - '3 years'::INTERVAL) THEN TRUE\n",
    "else FALSE\n",
    "end as patron_is_active,\n",
    "p.ptype_code as patron_ptype_code,\n",
    "p.home_library_code as patron_home_library_code,\n",
    "p.mblock_code as patron_mblock_code,\n",
    "case \n",
    "when p.owed_amt > 10.00 then TRUE\n",
    "else FALSE\n",
    "end as patron_has_over_10usd_owed\n",
    "from\n",
    "sierra_view.hold as h\n",
    "join sierra_view.record_metadata as r on r.id = h.record_id\n",
    "left outer join sierra_view.patron_record as p on p.record_id = h.patron_record_id\n",
    "\n",
    "order by\n",
    "hold_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'hold', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'hold_id': Integer(),\n",
    "        'bib_record_num': Integer(),\n",
    "        'item_record_num': Integer(),\n",
    "        'volume_record_num': Integer(),\n",
    "        'patron_ptype_code': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX pk_hold ON hold (hold_id);\n",
    "CREATE UNIQUE INDEX uc_hold_composite ON hold (hold_id, bib_record_num, placed_gmt);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_items\n",
    "# \n",
    "# create the tables for active items as defined here:\n",
    "# https://ilsweb.cincinnatilibrary.org/collection-analysis-docs/static_queries_holds.html#defining-active-items\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE TABLE IF NOT EXISTS active_items (\n",
    "    bib_record_num BIGINT,\n",
    "    item_record_num BIGINT,\n",
    "    volume_record_num BIGINT,\n",
    "    volume_statement TEXT,\n",
    "    items_display_order INTEGER\n",
    ");\n",
    "\n",
    "with active_items_data as (\n",
    "  -- \"active items\"\n",
    "  -- --------------\n",
    "  -- This will produce a list of items meeting the following criteria:\n",
    "  -- * item status is one of the following codes:\n",
    "  --   ('-', '!', 'b', 'p', '(', '@', ')', '_', '=', '+', 't')\n",
    "  -- * if the item has a due date, then it must be less than 60 days overdue:\n",
    "  --   coalesce( (julianday(date('now')) - julianday(item.due_date) > 60.0 ), FALSE)\n",
    "  select\n",
    "    item.bib_record_num,\n",
    "    item.item_record_num,\n",
    "    v.volume_record_num,\n",
    "    v.volume_statement,\n",
    "    v.items_display_order\n",
    "  from\n",
    "    item\n",
    "    left outer join volume_record_item_record_link as v on v.item_record_num = item.item_record_num -- we need to consider volume information for volume-level holds\n",
    "    join record_metadata as r on (\n",
    "      r.record_type_code = 'b'\n",
    "      and r.record_num = item.bib_record_num\n",
    "      and r.campus_code = ''\n",
    "    ) -- considers only items belonging to us (no virtual items)\n",
    "  where\n",
    "    -- * item status is one of the following codes:\n",
    "    --   ('-', '!', 'b', 'p', '(', '@', ')', '_', '=', '+', 't')\n",
    "    item.item_status_code in (\n",
    "      '-',\n",
    "      '!',\n",
    "      'b',\n",
    "      'p',\n",
    "      '(',\n",
    "      '@',\n",
    "      ')',\n",
    "      '_',\n",
    "      '=',\n",
    "      '+',\n",
    "      't'\n",
    "    ) -- * if the item has a due date, then it must be less than 60 days overdue:\n",
    "    --   coalesce( (julianday(date('now')) - julianday(item.due_date) > 60.0 ), FALSE)\n",
    "    and coalesce(\n",
    "      (\n",
    "        julianday(date('now')) - julianday(item.due_date) > 60.0\n",
    "      ),\n",
    "      FALSE\n",
    "    ) is FALSE\n",
    ")\n",
    "\n",
    "INSERT OR IGNORE INTO active_items (\n",
    "    bib_record_num,\n",
    "    item_record_num,\n",
    "    volume_record_num,\n",
    "    volume_statement,\n",
    "    items_display_order\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  active_items_data\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "        \n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_active_items_bib_record_num ON active_items (bib_record_num);\n",
    "CREATE INDEX idx_active_items_item_record_num ON active_items (item_record_num);\n",
    "CREATE INDEX idx_active_items_volume_record_num ON active_items (volume_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_holds\n",
    "# \n",
    "# create the tables for active items as defined here:\n",
    "# https://ilsweb.cincinnatilibrary.org/collection-analysis-docs/static_queries_holds.html#defining-active-holds\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE TABLE IF NOT EXISTS active_holds (\n",
    "    hold_id BIGINT\n",
    ");\n",
    "\n",
    "\n",
    "with active_holds_data as (\n",
    "  -- \"active holds\"\n",
    "  -- --------------\n",
    "  -- This will produce a list of holds meeting the following criteria:\n",
    "  -- * hold that is not Frozen (except for holds placed by patrons with ptype 196)\n",
    "  -- * hold with zero delay days OR the hold delay has passed (hold placed date + delay days is not a date in the future)\n",
    "  -- * hold placed by patron with one of the following ptype codes:\n",
    "  --   ( 0, 1, 2, 5, 6, 10, 11, 12, 15, 22, 30, 31, 32, 40, 41, 196 )\n",
    "  -- * hold status is \"on hold\"\n",
    "  select\n",
    "    h.hold_id\n",
    "  from\n",
    "    hold as h\n",
    "    join record_metadata as r on (\n",
    "      -- TODO figure out if maybe we could just use the `is_ill` boolean value to do this (this is still fast since it's an indexed search)\n",
    "      r.record_type_code = 'b'\n",
    "      and r.record_num = h.bib_record_num\n",
    "      and r.campus_code = ''\n",
    "    ) -- join the record metadata so that we're only concerning ourselves with titles that belong to us (to filter out ILL holds)\n",
    "  where\n",
    "    -- * hold that is not Frozen (except for holds placed by patrons with ptype 196)\n",
    "    (\n",
    "      h.is_frozen is FALSE\n",
    "      OR h.patron_ptype_code = 196\n",
    "    )\n",
    "    AND -- * hold with zero delay days OR the hold delay has passed (hold placed date + delay days is not in the future)\n",
    "    (\n",
    "      julianday(datetime('now')) - (\n",
    "        julianday(h.placed_gmt) + (h.delay_days * 1.0)\n",
    "      )\n",
    "    ) > 0\n",
    "    AND -- * hold placed by patron with one of the following ptype codes:\n",
    "    --   ( 0, 1, 2, 5, 6, 10, 11, 12, 15, 22, 30, 31, 32, 40, 41, 196 )\n",
    "    h.patron_ptype_code IN (\n",
    "      0,\n",
    "      1,\n",
    "      2,\n",
    "      5,\n",
    "      6,\n",
    "      10,\n",
    "      11,\n",
    "      12,\n",
    "      15,\n",
    "      22,\n",
    "      30,\n",
    "      31,\n",
    "      32,\n",
    "      40,\n",
    "      41,\n",
    "      196\n",
    "    )\n",
    "    AND -- * hold status is \"on hold\"\n",
    "    h.hold_status = 'on hold'\n",
    ")\n",
    "\n",
    "INSERT OR IGNORE INTO active_holds (\n",
    "    hold_id\n",
    ")\n",
    "\n",
    "select\n",
    "  *\n",
    "from\n",
    "  active_holds_data\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "        \n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_active_holds_holds_id ON active_holds (hold_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rsync -Pav current_collection.db plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/collection-2021-04-12.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table bib (bib_record_num, creation_date, record_last_updated, isbn, best_author, best_title, publisher, publish_year, bib_level_callnumber, indexed_subjects)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the full text search (fts) on the \n",
    "# best_author, best_title,\n",
    "# publisher, publish_year,\n",
    "# bib_level_callnumber, indexed_subjects\n",
    "# columns using the \n",
    "\n",
    "utils_db = sqlite_utils.Database('current_collection.db')\n",
    "utils_db[\"bib\"].enable_fts([\"best_author\", \"best_title\", \"publisher\", \"publish_year\", \"bib_level_callnumber\", \"indexed_subjects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bib_fts'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to double check if the table now has fts enabled ...\n",
    "utils_db[\"bib\"].detect_fts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*:\n",
    "\n",
    "Right now, I'm managing the old snapshots in a folder on the ilsaux2 server `/home/plchuser/output/collection-analysis/datasette_hosted_databases/`\n",
    "\n",
    "* * * \n",
    "\n",
    "**from the ilsaux2 server:**\n",
    "\n",
    "```bash\n",
    "rsync -Pav \\\n",
    "    plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/ \\\n",
    "    /home/plchuser/output/collection-analysis/datasette_hosted_databases/\n",
    "```\n",
    "\n",
    "* * * \n",
    "\n",
    "Next, you may have to remove the oldest snapshot to make room on the `ilsweb` server\n",
    "\n",
    "**from the ilsweb server:**\n",
    "\n",
    "for example:\n",
    "\n",
    "```bash\n",
    "rm collection-2021-03-22.db\n",
    "```\n",
    "\n",
    "* * * \n",
    "\n",
    "sync the output from this script \n",
    "\n",
    "**from the ilsaux2 server:**\n",
    "\n",
    "for example ...\n",
    "\n",
    "```bash\n",
    "rsync -Pav /home/plchuser/output/jupyter/collection-analysis/current_collection.db plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/collection-2021-08-23.db\n",
    "```\n",
    "\n",
    "* * *\n",
    "\n",
    "link the correct files:\n",
    "\n",
    "**from the ilsweb server:**\n",
    "\n",
    "```bash\n",
    "rm current_collection.db\n",
    "rm collection_prev.db\n",
    "```\n",
    "\n",
    "link the new files...\n",
    "\n",
    "```bash\n",
    "ln collection-2021-08-23.db current_collection.db\n",
    "ln collection-2021-08-16.db collection_prev.db\n",
    "```\n",
    "\n",
    "Edit the `metadata.yaml` file, and change the date\n",
    "\n",
    "```bash\n",
    "nano metadata.yaml\n",
    "```\n",
    "\n",
    "restart datasette (remember to reactivate the env)\n",
    "\n",
    "```text\n",
    "(venv) plchuser@ilsweb:~/data/collection-analysis$ ./start_datasette.sh\n",
    "```\n",
    "\n",
    "**Note**\n",
    "To remove a page from the htcache (if the page is stuck with an old cached version, run this command to clean it:\n",
    "\n",
    "```bash\n",
    "sudo htcacheclean -v -p/var/cache/apache2/mod_cache_disk/ \"https://ilsweb.cincinnatilibrary.org:443/collection-analysis/?\"`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
