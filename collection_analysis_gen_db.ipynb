{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (21.1.3)\n",
      "Requirement already satisfied: sqlite-utils in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (3.12)\n",
      "Requirement already satisfied: click in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (from sqlite-utils) (8.0.1)\n",
      "Requirement already satisfied: sqlite-fts4 in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (from sqlite-utils) (1.0.1)\n",
      "Requirement already satisfied: click-default-group in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (from sqlite-utils) (1.2.2)\n",
      "Requirement already satisfied: tabulate in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (from sqlite-utils) (0.8.9)\n",
      "Requirement already satisfied: importlib-metadata in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (from click->sqlite-utils) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (from importlib-metadata->click->sqlite-utils) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/plchuser/output/jupyter/venv/lib/python3.7/site-packages (from importlib-metadata->click->sqlite-utils) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U sqlite-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!!!\n",
    "# make sure that you run the backup first!\n",
    "# /home/plchuser/.backup/plch-ilsaux2-collection-analysis.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Integer, BigInteger, Numeric\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from decimal import Decimal\n",
    "import sqlite_utils\n",
    "\n",
    "import vars\n",
    "\n",
    "engine = create_engine('sqlite:///current_collection.db', echo=False)\n",
    "\n",
    "sierra_engine = create_engine('postgres://{}:{}@sierra-db.plch.net:1032/iii'.format(vars.pg_username, vars.pg_password))\n",
    "\n",
    "collection_file_path = os.path.join(os.getcwd(), '/home/plchuser/output/collection-analysis/')\n",
    "\n",
    "item_re = re.compile(r\"^[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}\\-plch\\-item\\.csv\\.xz\")\n",
    "bib_re = re.compile(r\"^[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}\\-plch\\-bib\\.csv\\.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n"
     ]
    }
   ],
   "source": [
    "# this is our working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE and refresh the sqlite database file in the local directory\n",
    "try:\n",
    "    os.remove('current_collection.db')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.close(os.open('current_collection.db', os.O_CREAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_files = os.listdir(collection_file_path)\n",
    "collection_files.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert price to integer values\n",
    "numbers_only_re = re.compile('[^0-9]')\n",
    "\n",
    "def price_to_int(price):\n",
    "    return int(numbers_only_re.sub('', price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-19-plch-item.csv.xz\n"
     ]
    }
   ],
   "source": [
    "item_file = [file for file in collection_files if item_re.match(file)][0]\n",
    "print(item_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was here to generate the database for the first snapshot of the year\n",
    "# item_file = '2020-01-06-plch-item.csv.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(collection_file_path, item_file),\n",
    "    compression='xz',\n",
    "    delimiter='|',\n",
    "    converters={'price': price_to_int},\n",
    "    # nrows=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'price': 'price_cents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name='item', index=False, if_exists='replace', con=engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-19-plch-bib.csv.xz\n"
     ]
    }
   ],
   "source": [
    "bib_file = [file for file in collection_files if bib_re.match(file)][0]\n",
    "print(bib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was here to generate the database for the first snapshot of the year\n",
    "# bib_file = '2020-01-06-plch-bib.csv.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(collection_file_path, bib_file),\n",
    "    compression='xz',\n",
    "    delimiter='|',\n",
    ")\n",
    "\n",
    "df.to_sql(\n",
    "    name='bib', \n",
    "    index=False, \n",
    "    if_exists='replace', \n",
    "    con=engine, \n",
    "    chunksize=10000,\n",
    "    dtype={\n",
    "        'publish_year': Integer(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes to create\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX IF NOT EXISTS \"idx_bib_bib_record_num\" ON \"bib\" (\n",
    "    \"bib_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_bib_indexed_subjects\" on bib (\n",
    "    \"indexed_subjects\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_format_item_status_location_code_item_callnumber\" ON \"item\" (\n",
    "    \"location_code\",\n",
    "    \"item_format\",\n",
    "    \"item_status_code\",\n",
    "    \"item_callnumber\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_bib_record_num\" ON \"item\" (\n",
    "    \"bib_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_record_num\" ON \"item\" (\n",
    "    \"item_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_agency_code_num_location_code\" ON \"item\" (\n",
    "    \"agency_code_num\",\n",
    "    \"location_code\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_barcode\" ON \"item\" (\n",
    "    \"barcode\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_format\" ON \"item\" (\n",
    "    \"item_format\"\n",
    ");\n",
    "\"\"\"\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000, 1000000 1000000, 2000000 18964, 3000000 0, "
     ]
    }
   ],
   "source": [
    "# bib_record\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "b.id,\n",
    "b.record_id,\n",
    "b.language_code,\n",
    "b.bcode1,\n",
    "b.bcode2,\n",
    "b.bcode3,\n",
    "b.country_code,\n",
    "b.index_change_count,\n",
    "b.is_on_course_reserve,\n",
    "b.is_right_result_exact,\n",
    "b.allocation_rule_code,\n",
    "b.skip_num,\n",
    "b.cataloging_date_gmt,\n",
    "b.marc_type_code,\n",
    "b.is_suppressed\n",
    "\n",
    "FROM\n",
    "sierra_view.bib_record as b\n",
    "\n",
    "JOIN\n",
    "sierra_view.record_metadata as r on r.id = b.record_id\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "\n",
    "ORDER BY\n",
    "b.id\n",
    "\n",
    "LIMIT 1000000 OFFSET {}\n",
    "\"\"\"\n",
    "\n",
    "# start the offset at 0, then add 100000 to the offset\n",
    "offset = 0\n",
    "count = 0\n",
    "while (True):\n",
    "    df = pd.read_sql(sql=sql.format(offset), con=sierra_engine)        \n",
    "    print(offset, df.shape[0], sep=' ', end=', ')\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    df.to_sql(\n",
    "        name='bib_record',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'record_id': Integer(),\n",
    "            'index_change_count': Integer(),\n",
    "            'skip_num': Integer(),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    count += 1\n",
    "    offset += 1000000\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX bib_record_bcode3_idx ON bib_record (bcode3);\n",
    "CREATE INDEX bib_record_bib_level_idx ON bib_record (bcode1);\n",
    "CREATE INDEX bib_record_country_idx ON bib_record (country_code);\n",
    "CREATE INDEX bib_record_lang_idx ON bib_record (language_code);\n",
    "CREATE INDEX bib_record_material_type_idx ON bib_record (bcode2);\n",
    "CREATE UNIQUE INDEX bib_record_record_key ON bib_record (record_id);\n",
    "CREATE INDEX idx_bib_record_cataloging_date ON bib_record (cataloging_date_gmt);\n",
    "CREATE UNIQUE INDEX pk_bib_record ON bib_record (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language_property\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "p.id,\n",
    "p.code,\n",
    "p.display_order,\n",
    "n.name\n",
    "\n",
    "from\n",
    "sierra_view.language_property as p\n",
    "\n",
    "join\n",
    "sierra_view.language_property_name as n\n",
    "on n.language_property_id = p.id\n",
    "\n",
    "order by id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "df.to_sql(\n",
    "        name='language_property',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'display_order': Integer(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX language_property_code_key ON language_property (code);\n",
    "CREATE UNIQUE INDEX pk_language_property ON language_property (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sierra_view.record_metadata\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "r.id,\n",
    "r.record_type_code,\n",
    "r.record_num,\n",
    "date(r.creation_date_gmt) as creation_date_gmt,\n",
    "date(r.deletion_date_gmt) as deletion_date_gmt,\n",
    "r.campus_code,\n",
    "r.agency_code_num,\n",
    "r.record_last_updated_gmt\n",
    "\n",
    "FROM\n",
    "sierra_view.record_metadata as r\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "-- started grabbing the deleted record data 2021-03-15\n",
    "AND r.deletion_date_gmt IS NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j' ) -- bibliographic, item, volume\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine )\n",
    "\n",
    "# output to the sqlite db\n",
    "# NOTE: the first time through, we'll want to \"replace\" ... the second \"append\"\n",
    "df.to_sql(name='record_metadata', index=False, if_exists='replace', con=engine, chunksize=10000)\n",
    "\n",
    "# -- started grabbing the deleted record data 2021-03-15\n",
    "# doing this as the second part since it may exceeed our memory limits\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "r.id,\n",
    "r.record_type_code,\n",
    "r.record_num,\n",
    "date(r.creation_date_gmt) as creation_date_gmt,\n",
    "date(r.deletion_date_gmt) as deletion_date_gmt,\n",
    "r.campus_code,\n",
    "r.agency_code_num,\n",
    "r.record_last_updated_gmt\n",
    "\n",
    "FROM\n",
    "sierra_view.record_metadata as r\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "-- started grabbing the deleted record data 2021-03-15\n",
    "AND r.deletion_date_gmt IS NOT NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j' ) -- bibliographic, item, volume\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine )\n",
    "\n",
    "# output to the sqlite db\n",
    "df.to_sql(name='record_metadata', index=False, if_exists='append', con=engine, chunksize=10000)\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_record_metadata_id_record_last_updated ON record_metadata (id, record_last_updated_gmt);\n",
    "CREATE INDEX idx_record_metadata_record_creation_date_gmt ON record_metadata (creation_date_gmt);\n",
    "CREATE INDEX idx_record_metadata_record_num ON record_metadata (record_num);\n",
    "CREATE UNIQUE INDEX pk_record_id ON record_metadata (id);\n",
    "CREATE UNIQUE INDEX record_id_unique_constraint ON record_metadata (record_type_code, record_num, campus_code);\n",
    "CREATE INDEX record_metadata_last_modified ON record_metadata (record_last_updated_gmt, record_type_code, id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6016964\n",
      "         id  bib_record_id  bib_record_num  item_record_id  item_record_num  \\\n",
      "0   1481488   420908780724         1985716    450972577188          1011108   \n",
      "1   1489279   420908616008         1821000    450972584979          1018899   \n",
      "2  11451750   420907809642         1014634    450982078657         10512577   \n",
      "3  12148146   420907809576         1014568    450982706328         11140248   \n",
      "4  11791765   420907809080         1014072    450982382162         10816082   \n",
      "\n",
      "   items_display_order  bibs_display_order  \n",
      "0                  0.0                   0  \n",
      "1                  0.0                   0  \n",
      "2               1083.0                   0  \n",
      "3               7019.0                   0  \n",
      "4               1710.0                   0  \n"
     ]
    }
   ],
   "source": [
    "# bib_record_item_record_link\n",
    "sql = \"\"\"\n",
    "select \n",
    "l.id,\n",
    "l.bib_record_id,\n",
    "r.record_num as bib_record_num,\n",
    "l.item_record_id,\n",
    "ir.record_num as item_record_num,\n",
    "l.items_display_order,\n",
    "l.bibs_display_order\n",
    "\n",
    "from\n",
    "sierra_view.bib_record_item_record_link as l\n",
    "\n",
    "join sierra_view.record_metadata as r on r.id = l.bib_record_id\n",
    "\n",
    "join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'bib_record_item_record_link', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'bib_record_id': BigInteger(),\n",
    "        'bib_record_num': Integer(), \n",
    "        'item_record_id': BigInteger(),\n",
    "        'item_record_num': Integer(),\n",
    "        'items_display_order': Integer(),\n",
    "        'bibs_display_order': Integer(), \n",
    "    }\n",
    ")\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_bib_record_item_record_link_bib_record_id ON bib_record_item_record_link (bib_record_id);\n",
    "CREATE INDEX idx_bib_record_item_record_link_bib_record_num ON bib_record_item_record_link (bib_record_num);\n",
    "CREATE INDEX item_record_id_index ON bib_record_item_record_link (item_record_id);\n",
    "CREATE INDEX item_record_num_index ON bib_record_item_record_link (item_record_num);\n",
    "CREATE UNIQUE INDEX pk_bib_record_item_record_link ON bib_record_item_record_link (id);\n",
    "CREATE UNIQUE INDEX uc_bib_record_item_record_link ON bib_record_item_record_link (bib_record_id, item_record_id);\n",
    "CREATE INDEX ucn_bib_record_item_record_link ON bib_record_item_record_link (bib_record_num, item_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680713\n",
      "        id  volume_record_id  volume_record_num  item_record_id  \\\n",
      "0   358209      455267826131            1292755    450974557246   \n",
      "1   310769      455267788599            1255223    450974381522   \n",
      "2   249342      455267735960            1202584    450974131592   \n",
      "3  1249534      455267990013            1456637    450980250593   \n",
      "4  1635327      455268036961            1503585    450982043103   \n",
      "\n",
      "   item_record_num  items_display_order volume_statement  \n",
      "0          2991166                  NaN     1987 v.00.01  \n",
      "1          2815442                  NaN             v.09  \n",
      "2          2565512                  NaN             v.02  \n",
      "3          8684513                  NaN             v.17  \n",
      "4         10477023                  NaN             v.01  \n"
     ]
    }
   ],
   "source": [
    "# volume_record_item_record_link\n",
    "sql = \"\"\"\n",
    "select\n",
    "l.id,\n",
    "l.volume_record_id,\n",
    "vr.record_num as volume_record_num,\n",
    "l.item_record_id,\n",
    "ir.record_num as item_record_num,\n",
    "l.items_display_order,\n",
    "(\n",
    "    select\n",
    "    string_agg(v.field_content, ', ' order by occ_num)\n",
    "\n",
    "    from\n",
    "    sierra_view.varfield as v\n",
    "\n",
    "    where\n",
    "    v.record_id = l.volume_record_id\n",
    "    and v.varfield_type_code = 'v'\n",
    ") as volume_statement\n",
    "\n",
    "from\n",
    "sierra_view.volume_record_item_record_link as l\n",
    "join sierra_view.record_metadata as vr on vr.id = l.volume_record_id\n",
    "join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'volume_record_item_record_link', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'volume_record_id': BigInteger(),\n",
    "        'volume_record_num': Integer(), \n",
    "        'item_record_id': BigInteger(),\n",
    "        'item_record_num': Integer(),\n",
    "        'items_display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_volume_record_item_record_link_volume_record_id ON volume_record_item_record_link (volume_record_id);\n",
    "CREATE INDEX idx_volume_record_item_record_link_volume_record_num ON volume_record_item_record_link (volume_record_num);\n",
    "CREATE UNIQUE INDEX pk_volume_record_item_record_link ON volume_record_item_record_link (id);\n",
    "CREATE UNIQUE INDEX uc_volume_record_item_record_link ON volume_record_item_record_link (item_record_id);\n",
    "CREATE INDEX volume_record_item_record_link_item_id_volume_id ON volume_record_item_record_link (item_record_id, volume_record_id);\n",
    "CREATE INDEX volume_record_item_record_link_item_num_volume_num ON volume_record_item_record_link (item_record_num, volume_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000, 1000000 1000000, 2000000 1000000, 3000000 1000000, 4000000 1000000, 5000000 1000000, 6000000 1000000, 7000000 63431, 8000000 0, "
     ]
    }
   ],
   "source": [
    "# phrase_entry\n",
    "\n",
    "# TODO\n",
    "# we're going to skip this for now, since there doesn't seem to be a big benefit to this\n",
    "\n",
    "# target these index_tag values:\n",
    "# \"d\": \"subject\"\n",
    "# \"a\": \"author\",\n",
    "# \"t\": \"title\",\n",
    "# \"o\": \"ocolc\",\n",
    "# \"c\": \"callnumber\",\n",
    "# \"i\": \"isbn\",\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT \n",
    "e.id,\n",
    "e.record_id,\n",
    "e.index_tag,\n",
    "e.varfield_type_code,\n",
    "e.occurrence,\n",
    "e.is_permuted,\n",
    "e.type2,\n",
    "e.type3,\n",
    "e.index_entry,\n",
    "e.insert_title,\n",
    "e.phrase_rule_rule_num,\n",
    "e.phrase_rule_operation,\n",
    "e.phrase_rule_subfield_list,\n",
    "e.original_content,\n",
    "e.parent_record_id,\n",
    "e.insert_title_tag,\n",
    "e.insert_title_occ\n",
    "\n",
    "FROM sierra_view.phrase_entry as e\n",
    "\n",
    "JOIN\n",
    "sierra_view.record_metadata as r\n",
    "ON\n",
    "  r.id = e.record_id\n",
    "  \n",
    "WHERE\n",
    "e.index_tag in (\n",
    "    'd'\n",
    "    -- add these back later maybe\n",
    "    -- , 'a', 't', 'o', 'c', 'i'\n",
    ")\n",
    "AND r.campus_code = ''\n",
    "AND r.deletion_date_gmt IS NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j') -- bibliographic, item, volume\n",
    "\n",
    "ORDER BY\n",
    "id\n",
    "\n",
    "LIMIT 1000000 OFFSET {}\n",
    "\"\"\"\n",
    "\n",
    "# start the offset at 0, then add 100000 to the offset\n",
    "offset = 0\n",
    "count = 0\n",
    "while (True):\n",
    "    df = pd.read_sql(sql=sql.format(offset), con=sierra_engine)        \n",
    "    print(offset, df.shape[0], sep=' ', end=', ')\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    df.to_sql(\n",
    "        name='phrase_entry',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'record_id': Integer(),\n",
    "            'occurrence': Integer(),\n",
    "            'type2': Integer(),\n",
    "            'phrase_rule_rule_num': Integer(),\n",
    "            'parent_record_id': Integer(),\n",
    "            'insert_title_occ': Integer(),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    count += 1\n",
    "    offset += 1000000\n",
    "    \n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_phrase_entry ON phrase_entry (((index_tag || index_entry)), type2, insert_title, record_id);\n",
    "CREATE INDEX idx_phrase_entry_parent_record_id ON phrase_entry (parent_record_id);\n",
    "CREATE INDEX idx_phrase_entry_record ON phrase_entry (record_id);\n",
    "CREATE INDEX idx_phrase_entry_record_key ON phrase_entry (record_id);\n",
    "CREATE UNIQUE INDEX pk_phrase_entry ON phrase_entry (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2981\n",
      "     id   code  branch_code_num parent_location_code  is_public  \\\n",
      "0   884  fotab             14.0                 None      False   \n",
      "1    81  y0604              1.0                 None      False   \n",
      "2  1683  nsjdn             29.0                 None      False   \n",
      "3    27  avjpl              3.0                 None      False   \n",
      "4   616  crzzz              9.0                 None      False   \n",
      "\n",
      "   is_requestable  \n",
      "0            True  \n",
      "1            True  \n",
      "2            True  \n",
      "3            True  \n",
      "4            True  \n"
     ]
    }
   ],
   "source": [
    "# location\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "code,\n",
    "branch_code_num,\n",
    "parent_location_code,\n",
    "is_public,\n",
    "is_requestable\n",
    "FROM\n",
    "sierra_view.location;\n",
    ";\n",
    "\"\"\"\n",
    "    \n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code': Integer(),\n",
    "        'branch_code_num': Integer(),\n",
    "        'parent_location_code': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk9ff58fb55804fddb ON location (branch_code_num);\n",
    "CREATE UNIQUE INDEX location_code_key ON location (code);\n",
    "CREATE UNIQUE INDEX pk_location ON location (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "   branch_id          name  iii_language_id\n",
      "0          1  Main Library                1\n",
      "1          2      Anderson                1\n",
      "2          3      Avondale                1\n",
      "3          4      Blue Ash                1\n",
      "4          5     Bond Hill                1\n"
     ]
    }
   ],
   "source": [
    "# branch_name\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "branch_id,\n",
    "name,\n",
    "iii_language_id\n",
    "FROM\n",
    "sierra_view.branch_name;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'branch_name', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'branch_id': Integer(),\n",
    "        'iii_language_id': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX branch_name_pkey ON branch_name (branch_id, iii_language_id);\n",
    "CREATE INDEX fk46f5c7085804fddb ON branch_name (branch_id);\n",
    "CREATE INDEX fk46f5c7088eaffe82 ON branch_name (iii_language_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "   id                                            address  \\\n",
      "0  52                                                      \n",
      "1  50  Main Library$800 Vine Street$Cincinnati, Ohio ...   \n",
      "2  17  Greenhills Branch$8 Enfield St.$Cincinnati, Oh...   \n",
      "3  49                                                      \n",
      "4  44                                                      \n",
      "\n",
      "                          email_source                       email_reply_to  \\\n",
      "0                                                                             \n",
      "1  patronnotices@cincinnatilibrary.org  patronnotices@cincinnatilibrary.org   \n",
      "2  patronnotices@cincinnatilibrary.org  patronnotices@cincinnatilibrary.org   \n",
      "3                                                                             \n",
      "4  patronnotices@cincinnatilibrary.org  patronnotices@cincinnatilibrary.org   \n",
      "\n",
      "  address_latitude address_longitude  code_num  \n",
      "0       39.1057790       -84.5133140        52  \n",
      "1       39.2305206       -84.3749388        50  \n",
      "2       39.2682526       -84.5221859        16  \n",
      "3             None              None        49  \n",
      "4             None              None        46  \n"
     ]
    }
   ],
   "source": [
    "# branch\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "address,\n",
    "email_source,\n",
    "email_reply_to,\n",
    "address_latitude,\n",
    "address_longitude,\n",
    "code_num\n",
    "\n",
    "FROM \n",
    "sierra_view.branch;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'branch', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code_num': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX pk_branch ON branch (id);\n",
    "CREATE UNIQUE INDEX uniq_branch_code_num ON branch (code_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "  code  display_order                      name\n",
      "0                   0                No country\n",
      "1   aa              1                   Albania\n",
      "2  abc              2                   Alberta\n",
      "3  aca              3  Australian Capital Terr.\n",
      "4   ae              4                   Algeria\n"
     ]
    }
   ],
   "source": [
    "# country_property_myuser\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    "from\n",
    "\n",
    "sierra_view.country_property_myuser\n",
    "\n",
    "order by code\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'country_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "  code  display_order                       name\n",
      "0    !              0               ON HOLDSHELF\n",
      "1    #              1       SearchOH/OL RECEIVED\n",
      "2    $              2              LOST AND PAID\n",
      "3    %              3       SearchOH/OL RETURNED\n",
      "4    &              4  SearchOH/OHIOLINK REQUEST\n"
     ]
    }
   ],
   "source": [
    "# item_status_property_myuser\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.item_status_property_myuser\n",
    "order by display_order\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'item_status_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "   code  display_order itype_property_category_id  physical_format_id  \\\n",
      "0   105             54                       None                 NaN   \n",
      "1     6              6                       None                 NaN   \n",
      "2   102             51                       None                 NaN   \n",
      "3   110             55                       None                 NaN   \n",
      "4     2              2                       None                 1.0   \n",
      "\n",
      "  target_audience_id                  name  \n",
      "0               None            Leased DVD  \n",
      "1               None           Leased Book  \n",
      "2               None                Bluray  \n",
      "3               None  MakerSpace Equipment  \n",
      "4               None         Juvenile Book  \n"
     ]
    }
   ],
   "source": [
    "# itype_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "code,\n",
    "display_order,\n",
    "itype_property_category_id,\n",
    "physical_format_id,\n",
    "target_audience_id,\n",
    "name\n",
    "\n",
    "FROM \n",
    "sierra_view.itype_property_myuser\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'itype_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'code': Integer(),\n",
    "        'display_order': Integer(),\n",
    "        'physical_format_id': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "   id  is_default  display_order         name\n",
      "0   1        True              1         Book\n",
      "1  29       False              2   Book on CD\n",
      "2  34       False              3          DVD\n",
      "3  35       False              4  Large Print\n",
      "4  33       False              5     Magazine\n"
     ]
    }
   ],
   "source": [
    "# physical_format_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.physical_format_myuser\n",
    "order by display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'physical_format_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "  code  display_order          name\n",
      "0    -              0           ---\n",
      "1    a              1  MONO COMP PT\n",
      "2    b              2   SER COMP PT\n",
      "3    c              3    COLLECTION\n",
      "4    d              4       SUBUNIT\n"
     ]
    }
   ],
   "source": [
    "# bib_level_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.bib_level_property_myuser\n",
    "\n",
    "order by\n",
    "display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'bib_level_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "  code  display_order  is_public material_property_category_id  \\\n",
      "0    -              0       True                          None   \n",
      "1    1              1       True                          None   \n",
      "2    2              2       True                          None   \n",
      "3    3              3       True                          None   \n",
      "4    4              4       True                          None   \n",
      "\n",
      "  physical_format_id                    name  \n",
      "0               None               Undefined  \n",
      "1               None  Downloadable Audiobook  \n",
      "2               None       Downloadable Book  \n",
      "3               None      Downloadable Music  \n",
      "4               None      Downloadable Video  \n"
     ]
    }
   ],
   "source": [
    "# material_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.material_property_myuser\n",
    "\n",
    "order by\n",
    "display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'material_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2981\n",
      "     id   code  branch_code_num parent_location_code  is_public  \\\n",
      "0   884  fotab             14.0                 None      False   \n",
      "1    81  y0604              1.0                 None      False   \n",
      "2  1683  nsjdn             29.0                 None      False   \n",
      "3    27  avjpl              3.0                 None      False   \n",
      "4   616  crzzz              9.0                 None      False   \n",
      "\n",
      "   is_requestable  \n",
      "0            True  \n",
      "1            True  \n",
      "2            True  \n",
      "3            True  \n",
      "4            True  \n"
     ]
    }
   ],
   "source": [
    "# location\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "code,\n",
    "branch_code_num,\n",
    "parent_location_code,\n",
    "is_public,\n",
    "is_requestable\n",
    "FROM\n",
    "sierra_view.location;\n",
    ";\n",
    "\"\"\"\n",
    "    \n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code': Integer(),\n",
    "        'branch_code_num': Integer(),\n",
    "        'parent_location_code': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk9ff58fb55804fddb ON location (branch_code_num);\n",
    "CREATE UNIQUE INDEX location_code_key ON location (code);\n",
    "CREATE UNIQUE INDEX pk_location ON location (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2981\n",
      "   location_id                                 name  iii_language_id\n",
      "0          884          Forest Park Teen Audiobooks                1\n",
      "1         2177         Walnut Hills Television DVDs                1\n",
      "2         1683  Northside Juvenile New Release DVDs                1\n",
      "3         2338                Wyoming Foreign Films                1\n",
      "4          616                   Corryville Cleanup                1\n"
     ]
    }
   ],
   "source": [
    "# location_name\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "location_id,\n",
    "name,\n",
    "iii_language_id\n",
    "FROM sierra_view.location_name;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location_name', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'location_id': Integer(),\n",
    "        'iii_language_id': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk506824d5399f0cbb ON location_name (location_id);\n",
    "CREATE INDEX fk506824d58eaffe82 ON location_name (iii_language_id);\n",
    "CREATE UNIQUE INDEX location_name_pkey ON location_name (location_id, iii_language_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158907\n",
      "   hold_id  bib_record_num campus_code record_type_on_hold  item_record_num  \\\n",
      "0  2707601         2621589                               j              NaN   \n",
      "1  2707604         2621589                               j              NaN   \n",
      "2  3473847         2621589                               j              NaN   \n",
      "3  3473848         2621589                               j              NaN   \n",
      "4  3473856         2823181                               j              NaN   \n",
      "\n",
      "   volume_record_num                 placed_gmt  is_frozen  delay_days  \\\n",
      "0          1386395.0  2013-01-21 16:11:15-05:00       True         255   \n",
      "1          1366848.0  2013-01-21 16:11:23-05:00       True         255   \n",
      "2          1386395.0  2013-03-11 19:09:43-04:00       True         255   \n",
      "3          1366848.0  2013-03-11 19:09:51-04:00       True         255   \n",
      "4          1401384.0  2013-03-11 19:10:11-04:00       True         255   \n",
      "\n",
      "  location_code  ... pickup_location_code ir_pickup_location_code  \\\n",
      "0          None  ...                   hp                    None   \n",
      "1          None  ...                   hp                    None   \n",
      "2          None  ...                   ch                    None   \n",
      "3          None  ...                   ch                    None   \n",
      "4          None  ...                   ch                    None   \n",
      "\n",
      "   ir_print_name  ir_delivery_stop_name is_ir_converted_request  \\\n",
      "0           None                   None                   False   \n",
      "1           None                   None                   False   \n",
      "2           None                   None                   False   \n",
      "3           None                   None                   False   \n",
      "4           None                   None                   False   \n",
      "\n",
      "  patron_is_active patron_ptype_code patron_home_library_code  \\\n",
      "0             True                 0                       hp   \n",
      "1             True                 0                       hp   \n",
      "2             True                 0                       pr   \n",
      "3             True                 0                       pr   \n",
      "4             True                 0                       pr   \n",
      "\n",
      "   patron_mblock_code  patron_has_over_10usd_owed  \n",
      "0                   -                       False  \n",
      "1                   -                       False  \n",
      "2                   -                       False  \n",
      "3                   -                       False  \n",
      "4                   -                       False  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# hold\n",
    "\n",
    "sql = \"\"\"\\\n",
    "-- pull all relevant hold data\n",
    "\n",
    "select\n",
    "h.id as hold_id,\n",
    "case\n",
    "    when r.record_type_code = 'i' then (\n",
    "        select\n",
    "        br.record_num\n",
    "        from\n",
    "        sierra_view.bib_record_item_record_link as l\n",
    "        join sierra_view.record_metadata as br on br.id = l.bib_record_id\n",
    "        where\n",
    "        l.item_record_id = h.record_id\n",
    "        limit 1\n",
    "    )\n",
    "    when r.record_type_code = 'j' then (\n",
    "        select\n",
    "        br.record_num\n",
    "        from\n",
    "        sierra_view.bib_record_volume_record_link as l\n",
    "        join sierra_view.record_metadata as br on br.id = l.bib_record_id\n",
    "        where\n",
    "        l.volume_record_id = h.record_id\n",
    "        limit 1\n",
    "    )\n",
    "    when r.record_type_code = 'b' then r.record_num\n",
    "    else NULL\n",
    "end as bib_record_num,\n",
    "r.campus_code,\n",
    "r.record_type_code as record_type_on_hold,\n",
    "case\n",
    "    when r.record_type_code = 'i' then r.record_num\n",
    "-- i don't think this is really useful, but i may want to come back to this \n",
    "-- when r.record_type_code = 'j' then (\n",
    "-- select\n",
    "-- ir.record_num\n",
    "-- from\n",
    "-- sierra_view.volume_record_item_record_link as l\n",
    "-- join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "-- where\n",
    "-- l.volume_record_id = h.record_id\n",
    "-- limit 1\n",
    "-- )\n",
    "    else NULL\n",
    "end as item_record_num,\n",
    "case\n",
    "    when r.record_type_code = 'j' then r.record_num\n",
    "    else NULL\n",
    "end as volume_record_num,\n",
    "h.placed_gmt,\n",
    "h.is_frozen,\n",
    "h.delay_days,\n",
    "h.location_code,\n",
    "h.expires_gmt,\n",
    "case\n",
    "when h.status = '0' then 'on hold'\n",
    "when h.status = 'b' then 'bib hold ready for pickup'\n",
    "when h.status = 'j' then 'volume hold ready for pickup'\n",
    "when h.status = 'i' then 'item hold ready for pickup'\n",
    "when h.status = 't' then 'in transit to pickup location'\n",
    "else h.status\n",
    "end as hold_status,\n",
    "h.is_ir,\n",
    "h.is_ill,\n",
    "h.pickup_location_code,\n",
    "h.ir_pickup_location_code,\n",
    "h.ir_print_name,\n",
    "h.ir_delivery_stop_name,\n",
    "h.is_ir_converted_request,\n",
    "case\n",
    "when p.activity_gmt >= (NOW() - '3 years'::INTERVAL) THEN TRUE\n",
    "else FALSE\n",
    "end as patron_is_active,\n",
    "p.ptype_code as patron_ptype_code,\n",
    "p.home_library_code as patron_home_library_code,\n",
    "p.mblock_code as patron_mblock_code,\n",
    "case \n",
    "when p.owed_amt > 10.00 then TRUE\n",
    "else FALSE\n",
    "end as patron_has_over_10usd_owed\n",
    "from\n",
    "sierra_view.hold as h\n",
    "join sierra_view.record_metadata as r on r.id = h.record_id\n",
    "left outer join sierra_view.patron_record as p on p.record_id = h.patron_record_id\n",
    "\n",
    "order by\n",
    "hold_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'hold', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'hold_id': Integer(),\n",
    "        'bib_record_num': Integer(),\n",
    "        'item_record_num': Integer(),\n",
    "        'vol_record_num': Integer(),\n",
    "        'patron_ptype_code': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX pk_hold ON hold (hold_id);\n",
    "CREATE UNIQUE INDEX uc_hold_composite ON hold (hold_id, bib_record_num, placed_gmt);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rsync -Pav current_collection.db plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/collection-2021-04-12.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table bib (bib_record_num, creation_date, record_last_updated, isbn, best_author, best_title, publisher, publish_year, bib_level_callnumber, indexed_subjects)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the full text search (fts) on the \n",
    "# best_author, best_title,\n",
    "# publisher, publish_year,\n",
    "# bib_level_callnumber, indexed_subjects\n",
    "# columns using the \n",
    "\n",
    "utils_db = sqlite_utils.Database('current_collection.db')\n",
    "utils_db[\"bib\"].enable_fts([\"best_author\", \"best_title\", \"publisher\", \"publish_year\", \"bib_level_callnumber\", \"indexed_subjects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bib_fts'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to double check if the table now has fts enabled ...\n",
    "utils_db[\"bib\"].detect_fts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
