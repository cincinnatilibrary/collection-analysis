{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!!!\n",
    "# make sure that you run the backup first!\n",
    "# /home/plchuser/.backup/plch-ilsaux2-collection-analysis.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note \n",
    "make sure to run the backup script first ...\n",
    "\n",
    "`/home/plchuser/.backup/plch-ilsaux2-collection-analysis.sh`\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# backup for collection analysis files for local and collectionHQ\n",
    "\n",
    "unset HISTFILE\n",
    "export B2_ACCOUNT_ID=\"\"\n",
    "export B2_ACCOUNT_KEY=\"\"\n",
    "export RESTIC_REPOSITORY=\"b2:plch-collection-analysis\"\n",
    "export RESTIC_PASSWORD=\"\"\n",
    "export RESTIC_CACHE_DIR=\"/home/plchuser/.cache/restic\"\n",
    "\n",
    "# we should only need to do this on an empty repo\n",
    "# /home/plchuser/.backup/restic/restic init\n",
    "\n",
    "cd /home/plchuser/output/collection-analysis\n",
    "/usr/bin/find *.csv -print0 | /usr/bin/xargs -0 xz -9 -T0\n",
    "\n",
    "# add --verbose after restic command for more info\n",
    "/home/plchuser/.backup/restic/restic \\\n",
    "        backup \\\n",
    "        /home/plchuser/output/collection-analysis\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n",
      "/home/plchuser/output/jupyter/collection-analysis/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.7/site-packages (21.3.1)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.7/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./venv/lib/python3.7/site-packages (from pandas) (1.21.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./venv/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sqlalchemy in ./venv/lib/python3.7/site-packages (1.4.26)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.27-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 2.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in ./venv/lib/python3.7/site-packages (from sqlalchemy) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata in ./venv/lib/python3.7/site-packages (from sqlalchemy) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./venv/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy) (3.5.0)\n",
      "Installing collected packages: sqlalchemy\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.26\n",
      "    Uninstalling SQLAlchemy-1.4.26:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.26\n",
      "Successfully installed sqlalchemy-1.4.27\n",
      "Requirement already satisfied: psycopg2-binary in ./venv/lib/python3.7/site-packages (2.9.1)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "     |████████████████████████████████| 3.0 MB 1.9 MB/s            \n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "  Attempting uninstall: psycopg2-binary\n",
      "    Found existing installation: psycopg2-binary 2.9.1\n",
      "    Uninstalling psycopg2-binary-2.9.1:\n",
      "      Successfully uninstalled psycopg2-binary-2.9.1\n",
      "Successfully installed psycopg2-binary-2.9.2\n",
      "Requirement already satisfied: sqlite-utils in ./venv/lib/python3.7/site-packages (3.17.1)\n",
      "Requirement already satisfied: sqlite-fts4 in ./venv/lib/python3.7/site-packages (from sqlite-utils) (1.0.1)\n",
      "Requirement already satisfied: dateutils in ./venv/lib/python3.7/site-packages (from sqlite-utils) (0.6.12)\n",
      "Requirement already satisfied: click-default-group in ./venv/lib/python3.7/site-packages (from sqlite-utils) (1.2.2)\n",
      "Requirement already satisfied: tabulate in ./venv/lib/python3.7/site-packages (from sqlite-utils) (0.8.9)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.7/site-packages (from sqlite-utils) (8.0.1)\n",
      "Requirement already satisfied: importlib-metadata in ./venv/lib/python3.7/site-packages (from click->sqlite-utils) (3.10.1)\n",
      "Requirement already satisfied: pytz in ./venv/lib/python3.7/site-packages (from dateutils->sqlite-utils) (2021.1)\n",
      "Requirement already satisfied: python-dateutil in ./venv/lib/python3.7/site-packages (from dateutils->sqlite-utils) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./venv/lib/python3.7/site-packages (from importlib-metadata->click->sqlite-utils) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata->click->sqlite-utils) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil->dateutils->sqlite-utils) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U pandas\n",
    "!pip install -U sqlalchemy\n",
    "!pip install -U psycopg2-binary\n",
    "!pip install -U sqlite-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Integer, BigInteger, Numeric\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from decimal import Decimal\n",
    "import sqlite_utils\n",
    "\n",
    "import vars\n",
    "\n",
    "engine = create_engine('sqlite:///current_collection.db', echo=False)\n",
    "\n",
    "sierra_engine = create_engine('postgresql://{}:{}@sierra-db.plch.net:1032/iii'.format(vars.pg_username, vars.pg_password))\n",
    "\n",
    "collection_file_path = os.path.join(os.getcwd(), '/home/plchuser/output/collection-analysis/')\n",
    "\n",
    "item_re = re.compile(r\"^[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}\\-plch\\-item\\.csv\\.xz\")\n",
    "bib_re = re.compile(r\"^[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}\\-plch\\-bib\\.csv\\.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/plchuser/output/jupyter/collection-analysis\n"
     ]
    }
   ],
   "source": [
    "# this is our working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE and refresh the sqlite database file in the local directory\n",
    "try:\n",
    "    os.remove('current_collection.db')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.close(os.open('current_collection.db', os.O_CREAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_files = os.listdir(collection_file_path)\n",
    "collection_files.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert price to integer values\n",
    "numbers_only_re = re.compile('[^0-9]')\n",
    "\n",
    "def price_to_int(price):\n",
    "    return int(numbers_only_re.sub('', price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-08-plch-item.csv.xz\n"
     ]
    }
   ],
   "source": [
    "item_file = [file for file in collection_files if item_re.match(file)][0]\n",
    "print(item_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was here to generate the database for the first snapshot of the year\n",
    "# item_file = '2020-01-06-plch-item.csv.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(collection_file_path, item_file),\n",
    "    compression='xz',\n",
    "    delimiter='|',\n",
    "    converters={'price': price_to_int},\n",
    "    # nrows=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'price': 'price_cents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name='item', index=False, if_exists='replace', con=engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-08-plch-bib.csv.xz\n"
     ]
    }
   ],
   "source": [
    "bib_file = [file for file in collection_files if bib_re.match(file)][0]\n",
    "print(bib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was here to generate the database for the first snapshot of the year\n",
    "# bib_file = '2020-01-06-plch-bib.csv.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(collection_file_path, bib_file),\n",
    "    compression='xz',\n",
    "    delimiter='|',\n",
    ")\n",
    "\n",
    "df.to_sql(\n",
    "    name='bib', \n",
    "    index=False, \n",
    "    if_exists='replace', \n",
    "    con=engine, \n",
    "    chunksize=10000,\n",
    "    dtype={\n",
    "        'publish_year': Integer(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes to create\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX IF NOT EXISTS \"idx_bib_bib_record_num\" ON \"bib\" (\n",
    "    \"bib_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_bib_indexed_subjects\" on bib (\n",
    "    \"indexed_subjects\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_format_item_status_location_code_item_callnumber\" ON \"item\" (\n",
    "    \"location_code\",\n",
    "    \"item_format\",\n",
    "    \"item_status_code\",\n",
    "    \"item_callnumber\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_bib_record_num\" ON \"item\" (\n",
    "    \"bib_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_record_num\" ON \"item\" (\n",
    "    \"item_record_num\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_agency_code_num_location_code\" ON \"item\" (\n",
    "    \"agency_code_num\",\n",
    "    \"location_code\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_barcode\" ON \"item\" (\n",
    "    \"barcode\"\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS \"idx_item_item_format\" ON \"item\" (\n",
    "    \"item_format\"\n",
    ");\n",
    "\"\"\"\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000, 1000000 1000000, 2000000 95570, 3000000 0, "
     ]
    }
   ],
   "source": [
    "# bib_record\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "b.id,\n",
    "b.record_id,\n",
    "b.language_code,\n",
    "b.bcode1,\n",
    "b.bcode2,\n",
    "b.bcode3,\n",
    "b.country_code,\n",
    "b.index_change_count,\n",
    "b.is_on_course_reserve,\n",
    "b.is_right_result_exact,\n",
    "b.allocation_rule_code,\n",
    "b.skip_num,\n",
    "b.cataloging_date_gmt,\n",
    "b.marc_type_code,\n",
    "b.is_suppressed\n",
    "\n",
    "FROM\n",
    "sierra_view.bib_record as b\n",
    "\n",
    "JOIN\n",
    "sierra_view.record_metadata as r on r.id = b.record_id\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "\n",
    "ORDER BY\n",
    "b.id\n",
    "\n",
    "LIMIT 1000000 OFFSET {}\n",
    "\"\"\"\n",
    "\n",
    "# start the offset at 0, then add 100000 to the offset\n",
    "offset = 0\n",
    "count = 0\n",
    "while (True):\n",
    "    df = pd.read_sql(sql=sql.format(offset), con=sierra_engine)        \n",
    "    print(offset, df.shape[0], sep=' ', end=', ')\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    df.to_sql(\n",
    "        name='bib_record',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'record_id': Integer(),\n",
    "            'index_change_count': Integer(),\n",
    "            'skip_num': Integer(),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    count += 1\n",
    "    offset += 1000000\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX bib_record_bcode3_idx ON bib_record (bcode3);\n",
    "CREATE INDEX bib_record_bib_level_idx ON bib_record (bcode1);\n",
    "CREATE INDEX bib_record_country_idx ON bib_record (country_code);\n",
    "CREATE INDEX bib_record_lang_idx ON bib_record (language_code);\n",
    "CREATE INDEX bib_record_material_type_idx ON bib_record (bcode2);\n",
    "CREATE UNIQUE INDEX bib_record_record_key ON bib_record (record_id);\n",
    "CREATE INDEX idx_bib_record_cataloging_date ON bib_record (cataloging_date_gmt);\n",
    "CREATE UNIQUE INDEX pk_bib_record ON bib_record (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language_property\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "p.id,\n",
    "p.code,\n",
    "p.display_order,\n",
    "n.name\n",
    "\n",
    "from\n",
    "sierra_view.language_property as p\n",
    "\n",
    "join\n",
    "sierra_view.language_property_name as n\n",
    "on n.language_property_id = p.id\n",
    "\n",
    "order by id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "df.to_sql(\n",
    "        name='language_property',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'display_order': Integer(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX language_property_code_key ON language_property (code);\n",
    "CREATE UNIQUE INDEX pk_language_property ON language_property (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sierra_view.record_metadata\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "r.id,\n",
    "r.record_type_code,\n",
    "r.record_num,\n",
    "date(r.creation_date_gmt) as creation_date_gmt,\n",
    "date(r.deletion_date_gmt) as deletion_date_gmt,\n",
    "r.campus_code,\n",
    "r.agency_code_num,\n",
    "r.record_last_updated_gmt\n",
    "\n",
    "FROM\n",
    "sierra_view.record_metadata as r\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "-- started grabbing the deleted record data 2021-03-15\n",
    "AND r.deletion_date_gmt IS NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j' ) -- bibliographic, item, volume\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine )\n",
    "\n",
    "# output to the sqlite db\n",
    "# NOTE: the first time through, we'll want to \"replace\" ... the second \"append\"\n",
    "df.to_sql(name='record_metadata', index=False, if_exists='replace', con=engine, chunksize=10000)\n",
    "\n",
    "# -- started grabbing the deleted record data 2021-03-15\n",
    "# doing this as the second part since it may exceeed our memory limits\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "r.id,\n",
    "r.record_type_code,\n",
    "r.record_num,\n",
    "date(r.creation_date_gmt) as creation_date_gmt,\n",
    "date(r.deletion_date_gmt) as deletion_date_gmt,\n",
    "r.campus_code,\n",
    "r.agency_code_num,\n",
    "r.record_last_updated_gmt\n",
    "\n",
    "FROM\n",
    "sierra_view.record_metadata as r\n",
    "\n",
    "WHERE\n",
    "r.campus_code = ''\n",
    "-- started grabbing the deleted record data 2021-03-15\n",
    "AND r.deletion_date_gmt IS NOT NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j' ) -- bibliographic, item, volume\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine )\n",
    "\n",
    "# output to the sqlite db\n",
    "df.to_sql(name='record_metadata', index=False, if_exists='append', con=engine, chunksize=10000)\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_record_metadata_id_record_last_updated ON record_metadata (id, record_last_updated_gmt);\n",
    "CREATE INDEX idx_record_metadata_record_creation_date_gmt ON record_metadata (creation_date_gmt);\n",
    "CREATE INDEX idx_record_metadata_record_num ON record_metadata (record_num);\n",
    "CREATE INDEX pk_record_id ON record_metadata (id);\n",
    "CREATE INDEX record_id_unique_constraint ON record_metadata (record_type_code, record_num, campus_code);\n",
    "CREATE INDEX record_metadata_last_modified ON record_metadata (record_last_updated_gmt, record_type_code, id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6190467\n",
      "         id  bib_record_id  bib_record_num  item_record_id  item_record_num  \\\n",
      "0   2300207   420908943241         2148233    450973395907          1829827   \n",
      "1   6966841   420909339758         2544750    450978062541          6496461   \n",
      "2   3077027   420908297409         1502401    450974172727          2606647   \n",
      "3   9821551   420910008622         3213614    450980647069          9080989   \n",
      "4  10644296   420910124993         3329985    450981374700          9808620   \n",
      "\n",
      "   items_display_order  bibs_display_order  \n",
      "0                  0.0                   0  \n",
      "1                  4.0                   0  \n",
      "2                  0.0                   0  \n",
      "3                  3.0                   0  \n",
      "4                 13.0                   0  \n"
     ]
    }
   ],
   "source": [
    "# bib_record_item_record_link\n",
    "sql = \"\"\"\n",
    "select \n",
    "l.id,\n",
    "l.bib_record_id,\n",
    "r.record_num as bib_record_num,\n",
    "l.item_record_id,\n",
    "ir.record_num as item_record_num,\n",
    "l.items_display_order,\n",
    "l.bibs_display_order\n",
    "\n",
    "from\n",
    "sierra_view.bib_record_item_record_link as l\n",
    "\n",
    "join sierra_view.record_metadata as r on r.id = l.bib_record_id\n",
    "\n",
    "join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'bib_record_item_record_link', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'bib_record_id': BigInteger(),\n",
    "        'bib_record_num': Integer(), \n",
    "        'item_record_id': BigInteger(),\n",
    "        'item_record_num': Integer(),\n",
    "        'items_display_order': Integer(),\n",
    "        'bibs_display_order': Integer(), \n",
    "    }\n",
    ")\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_bib_record_item_record_link_bib_record_id ON bib_record_item_record_link (bib_record_id);\n",
    "CREATE INDEX idx_bib_record_item_record_link_bib_record_num ON bib_record_item_record_link (bib_record_num);\n",
    "CREATE INDEX item_record_id_index ON bib_record_item_record_link (item_record_id);\n",
    "CREATE INDEX item_record_num_index ON bib_record_item_record_link (item_record_num);\n",
    "CREATE UNIQUE INDEX pk_bib_record_item_record_link ON bib_record_item_record_link (id);\n",
    "CREATE UNIQUE INDEX uc_bib_record_item_record_link ON bib_record_item_record_link (bib_record_id, item_record_id);\n",
    "CREATE INDEX ucn_bib_record_item_record_link ON bib_record_item_record_link (bib_record_num, item_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869947\n",
      "          id  volume_record_id  volume_record_num  item_record_id  \\\n",
      "0  1814429.0      455268052693            1519317    4.509830e+11   \n",
      "1  1331478.0      455267996430            1463054    4.509806e+11   \n",
      "2  1331480.0      455267996430            1463054    4.509806e+11   \n",
      "3  1662112.0      455267996430            1463054    4.509822e+11   \n",
      "4  1662111.0      455267996430            1463054    4.509822e+11   \n",
      "\n",
      "   item_record_num  items_display_order       volume_statement  \n",
      "0       11413961.0                  NaN  V. 273 NO. 2 Sep 2021  \n",
      "1        9064937.0                  NaN                   v.04  \n",
      "2        9064939.0                  NaN                   v.04  \n",
      "3       10605788.0                  NaN                   v.04  \n",
      "4       10605789.0                  NaN                   v.04  \n"
     ]
    }
   ],
   "source": [
    "# volume_record_item_record_link\n",
    "\n",
    "# NOTE, this version was used previously, but would't pull in volume info if there were no items (which happens alot)\n",
    "\"\"\"\n",
    "select\n",
    "l.id,\n",
    "l.volume_record_id,\n",
    "vr.record_num as volume_record_num,\n",
    "l.item_record_id,\n",
    "ir.record_num as item_record_num,\n",
    "l.items_display_order,\n",
    "(\n",
    "    select\n",
    "    string_agg(v.field_content, ', ' order by occ_num)\n",
    "\n",
    "    from\n",
    "    sierra_view.varfield as v\n",
    "\n",
    "    where\n",
    "    v.record_id = l.volume_record_id\n",
    "    and v.varfield_type_code = 'v'\n",
    ") as volume_statement\n",
    "\n",
    "from\n",
    "sierra_view.volume_record_item_record_link as l\n",
    "join sierra_view.record_metadata as vr on vr.id = l.volume_record_id\n",
    "join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "\"\"\"\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "l.id,\n",
    "r.id AS volume_record_id,\n",
    "r.record_num AS volume_record_num,\n",
    "ri.id AS item_record_id,\n",
    "ri.record_num AS item_record_num,\n",
    "l.items_display_order,\n",
    "(\n",
    "    SELECT\n",
    "    string_agg(v.field_content, ', ' ORDER BY occ_num)\n",
    "    FROM\n",
    "    sierra_view.varfield AS v\n",
    "    WHERE\n",
    "    v.record_id = r.id\n",
    "    AND v.varfield_type_code = 'v'\n",
    ") AS volume_statement\n",
    "FROM\n",
    "sierra_view.record_metadata AS r\n",
    "LEFT OUTER JOIN sierra_view.volume_record_item_record_link AS l ON\n",
    "l.volume_record_id = r.id\n",
    "LEFT OUTER JOIN sierra_view.record_metadata ri ON\n",
    "ri.id = l.item_record_id\n",
    "WHERE\n",
    "r.record_type_code = 'j'\n",
    "AND r.campus_code = ''\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'volume_record_item_record_link', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': pd.Int64Dtype(),\n",
    "        'volume_record_id': BigInteger(),\n",
    "        'volume_record_num': Integer(), \n",
    "        'item_record_id': BigInteger(),\n",
    "        'item_record_num': Integer(),\n",
    "        'items_display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_volume_record_item_record_link_volume_record_id ON volume_record_item_record_link (volume_record_id);\n",
    "CREATE INDEX idx_volume_record_item_record_link_volume_record_num ON volume_record_item_record_link (volume_record_num);\n",
    "CREATE UNIQUE INDEX pk_volume_record_item_record_link ON volume_record_item_record_link (id);\n",
    "CREATE UNIQUE INDEX uc_volume_record_item_record_link ON volume_record_item_record_link (item_record_id);\n",
    "CREATE INDEX volume_record_item_record_link_item_id_volume_id ON volume_record_item_record_link (item_record_id, volume_record_id);\n",
    "CREATE INDEX volume_record_item_record_link_item_num_volume_num ON volume_record_item_record_link (item_record_num, volume_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000, 1000000 1000000, 2000000 1000000, 3000000 1000000, 4000000 1000000, 5000000 1000000, 6000000 1000000, "
     ]
    }
   ],
   "source": [
    "# phrase_entry\n",
    "\n",
    "# TODO\n",
    "# we're going to skip this for now, since there doesn't seem to be a big benefit to this\n",
    "\n",
    "# target these index_tag values:\n",
    "# \"d\": \"subject\"\n",
    "# \"a\": \"author\",\n",
    "# \"t\": \"title\",\n",
    "# \"o\": \"ocolc\",\n",
    "# \"c\": \"callnumber\",\n",
    "# \"i\": \"isbn\",\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT \n",
    "e.id,\n",
    "e.record_id,\n",
    "e.index_tag,\n",
    "e.varfield_type_code,\n",
    "e.occurrence,\n",
    "e.is_permuted,\n",
    "e.type2,\n",
    "e.type3,\n",
    "e.index_entry,\n",
    "e.insert_title,\n",
    "e.phrase_rule_rule_num,\n",
    "e.phrase_rule_operation,\n",
    "e.phrase_rule_subfield_list,\n",
    "e.original_content,\n",
    "e.parent_record_id,\n",
    "e.insert_title_tag,\n",
    "e.insert_title_occ\n",
    "\n",
    "FROM sierra_view.phrase_entry as e\n",
    "\n",
    "JOIN\n",
    "sierra_view.record_metadata as r\n",
    "ON\n",
    "  r.id = e.record_id\n",
    "  \n",
    "WHERE\n",
    "e.index_tag in (\n",
    "    'd'\n",
    "    -- add these back later maybe\n",
    "    -- , 'a', 't', 'o', 'c', 'i'\n",
    ")\n",
    "AND r.campus_code = ''\n",
    "AND r.deletion_date_gmt IS NULL\n",
    "AND r.record_type_code in ('b', 'i', 'j') -- bibliographic, item, volume\n",
    "\n",
    "ORDER BY\n",
    "id\n",
    "\n",
    "LIMIT 1000000 OFFSET {}\n",
    "\"\"\"\n",
    "\n",
    "# start the offset at 0, then add 100000 to the offset\n",
    "offset = 0\n",
    "count = 0\n",
    "while (True):\n",
    "    df = pd.read_sql(sql=sql.format(offset), con=sierra_engine)        \n",
    "    print(offset, df.shape[0], sep=' ', end=', ')\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    df.to_sql(\n",
    "        name='phrase_entry',\n",
    "        con=engine,\n",
    "        index=False,\n",
    "        if_exists='append',\n",
    "        chunksize=10000, \n",
    "        dtype={\n",
    "            'id': Integer(),\n",
    "            'record_id': Integer(),\n",
    "            'occurrence': Integer(),\n",
    "            'type2': Integer(),\n",
    "            'phrase_rule_rule_num': Integer(),\n",
    "            'parent_record_id': Integer(),\n",
    "            'insert_title_occ': Integer(),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    count += 1\n",
    "    offset += 1000000\n",
    "    \n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_phrase_entry ON phrase_entry (((index_tag || index_entry)), type2, insert_title, record_id);\n",
    "CREATE INDEX idx_phrase_entry_parent_record_id ON phrase_entry (parent_record_id);\n",
    "CREATE INDEX idx_phrase_entry_record ON phrase_entry (record_id);\n",
    "CREATE INDEX idx_phrase_entry_record_key ON phrase_entry (record_id);\n",
    "CREATE UNIQUE INDEX pk_phrase_entry ON phrase_entry (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "code,\n",
    "branch_code_num,\n",
    "parent_location_code,\n",
    "is_public,\n",
    "is_requestable\n",
    "FROM\n",
    "sierra_view.location;\n",
    ";\n",
    "\"\"\"\n",
    "    \n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code': Integer(),\n",
    "        'branch_code_num': Integer(),\n",
    "        'parent_location_code': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk9ff58fb55804fddb ON location (branch_code_num);\n",
    "CREATE UNIQUE INDEX location_code_key ON location (code);\n",
    "CREATE UNIQUE INDEX pk_location ON location (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch_name\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "branch_id,\n",
    "name,\n",
    "iii_language_id\n",
    "FROM\n",
    "sierra_view.branch_name;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'branch_name', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'branch_id': Integer(),\n",
    "        'iii_language_id': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX branch_name_pkey ON branch_name (branch_id, iii_language_id);\n",
    "CREATE INDEX fk46f5c7085804fddb ON branch_name (branch_id);\n",
    "CREATE INDEX fk46f5c7088eaffe82 ON branch_name (iii_language_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "address,\n",
    "email_source,\n",
    "email_reply_to,\n",
    "address_latitude,\n",
    "address_longitude,\n",
    "code_num\n",
    "\n",
    "FROM \n",
    "sierra_view.branch;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'branch', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code_num': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX pk_branch ON branch (id);\n",
    "CREATE UNIQUE INDEX uniq_branch_code_num ON branch (code_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_property_myuser\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    "from\n",
    "\n",
    "sierra_view.country_property_myuser\n",
    "\n",
    "order by code\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'country_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_status_property_myuser\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.item_status_property_myuser\n",
    "order by display_order\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'item_status_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'display_order': Integer(),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itype_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "code,\n",
    "display_order,\n",
    "itype_property_category_id,\n",
    "physical_format_id,\n",
    "target_audience_id,\n",
    "name\n",
    "\n",
    "FROM \n",
    "sierra_view.itype_property_myuser\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'itype_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'code': Integer(),\n",
    "        'display_order': Integer(),\n",
    "        'physical_format_id': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_format_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.physical_format_myuser\n",
    "order by display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'physical_format_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bib_level_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.bib_level_property_myuser\n",
    "\n",
    "order by\n",
    "display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'bib_level_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# material_property_myuser\n",
    "\n",
    "sql = \"\"\"\\\n",
    "select\n",
    "*\n",
    "from\n",
    "sierra_view.material_property_myuser\n",
    "\n",
    "order by\n",
    "display_order\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'material_property_myuser', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'display_order': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "id,\n",
    "code,\n",
    "branch_code_num,\n",
    "parent_location_code,\n",
    "is_public,\n",
    "is_requestable\n",
    "FROM\n",
    "sierra_view.location;\n",
    ";\n",
    "\"\"\"\n",
    "    \n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'id': Integer(),\n",
    "        'code': Integer(),\n",
    "        'branch_code_num': Integer(),\n",
    "        'parent_location_code': Integer()\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk9ff58fb55804fddb ON location (branch_code_num);\n",
    "CREATE UNIQUE INDEX location_code_key ON location (code);\n",
    "CREATE UNIQUE INDEX pk_location ON location (id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_name\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "location_id,\n",
    "name,\n",
    "iii_language_id\n",
    "FROM sierra_view.location_name;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'location_name', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'location_id': Integer(),\n",
    "        'iii_language_id': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX fk506824d5399f0cbb ON location_name (location_id);\n",
    "CREATE INDEX fk506824d58eaffe82 ON location_name (iii_language_id);\n",
    "CREATE UNIQUE INDEX location_name_pkey ON location_name (location_id, iii_language_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold\n",
    "\n",
    "sql = \"\"\"\\\n",
    "-- pull all relevant hold data\n",
    "\n",
    "select\n",
    "h.id as hold_id,\n",
    "case\n",
    "    when r.record_type_code = 'i' then (\n",
    "        select\n",
    "        br.record_num\n",
    "        from\n",
    "        sierra_view.bib_record_item_record_link as l\n",
    "        join sierra_view.record_metadata as br on br.id = l.bib_record_id\n",
    "        where\n",
    "        l.item_record_id = h.record_id\n",
    "        limit 1\n",
    "    )\n",
    "    when r.record_type_code = 'j' then (\n",
    "        select\n",
    "        br.record_num\n",
    "        from\n",
    "        sierra_view.bib_record_volume_record_link as l\n",
    "        join sierra_view.record_metadata as br on br.id = l.bib_record_id\n",
    "        where\n",
    "        l.volume_record_id = h.record_id\n",
    "        limit 1\n",
    "    )\n",
    "    when r.record_type_code = 'b' then r.record_num\n",
    "    else NULL\n",
    "end as bib_record_num,\n",
    "r.campus_code,\n",
    "r.record_type_code as record_type_on_hold,\n",
    "case\n",
    "    when r.record_type_code = 'i' then r.record_num\n",
    "-- i don't think this is really useful, but i may want to come back to this \n",
    "-- when r.record_type_code = 'j' then (\n",
    "-- select\n",
    "-- ir.record_num\n",
    "-- from\n",
    "-- sierra_view.volume_record_item_record_link as l\n",
    "-- join sierra_view.record_metadata as ir on ir.id = l.item_record_id\n",
    "-- where\n",
    "-- l.volume_record_id = h.record_id\n",
    "-- limit 1\n",
    "-- )\n",
    "    else NULL\n",
    "end as item_record_num,\n",
    "case\n",
    "    when r.record_type_code = 'j' then r.record_num\n",
    "    else NULL\n",
    "end as volume_record_num,\n",
    "h.placed_gmt,\n",
    "h.is_frozen,\n",
    "h.delay_days,\n",
    "h.location_code,\n",
    "h.expires_gmt,\n",
    "case\n",
    "when h.status = '0' then 'on hold'\n",
    "when h.status = 'b' then 'bib hold ready for pickup'\n",
    "when h.status = 'j' then 'volume hold ready for pickup'\n",
    "when h.status = 'i' then 'item hold ready for pickup'\n",
    "when h.status = 't' then 'in transit to pickup location'\n",
    "else h.status\n",
    "end as hold_status,\n",
    "h.is_ir,\n",
    "h.is_ill,\n",
    "h.pickup_location_code,\n",
    "h.ir_pickup_location_code,\n",
    "h.ir_print_name,\n",
    "h.ir_delivery_stop_name,\n",
    "h.is_ir_converted_request,\n",
    "case\n",
    "when p.activity_gmt >= (NOW() - '3 years'::INTERVAL) THEN TRUE\n",
    "else FALSE\n",
    "end as patron_is_active,\n",
    "p.ptype_code as patron_ptype_code,\n",
    "p.home_library_code as patron_home_library_code,\n",
    "p.mblock_code as patron_mblock_code,\n",
    "case \n",
    "when p.owed_amt > 10.00 then TRUE\n",
    "else FALSE\n",
    "end as patron_has_over_10usd_owed\n",
    "from\n",
    "sierra_view.hold as h\n",
    "join sierra_view.record_metadata as r on r.id = h.record_id\n",
    "left outer join sierra_view.patron_record as p on p.record_id = h.patron_record_id\n",
    "\n",
    "order by\n",
    "hold_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql=sql, con=sierra_engine)\n",
    "\n",
    "# write results to sqlite db\n",
    "df.to_sql(\n",
    "    'hold', \n",
    "    con=engine, \n",
    "    index=False, \n",
    "    if_exists='replace',\n",
    "    dtype={\n",
    "        'hold_id': Integer(),\n",
    "        'bib_record_num': Integer(),\n",
    "        'item_record_num': Integer(),\n",
    "        'volume_record_num': Integer(),\n",
    "        'patron_ptype_code': Integer(),\n",
    "    },\n",
    "    chunksize=10000\n",
    ")\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE UNIQUE INDEX pk_hold ON hold (hold_id);\n",
    "CREATE UNIQUE INDEX uc_hold_composite ON hold (hold_id, bib_record_num, placed_gmt);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_items\n",
    "# \n",
    "# create the tables for active items as defined here:\n",
    "# https://ilsweb.cincinnatilibrary.org/collection-analysis-docs/static_queries_holds.html#defining-active-items\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE TABLE IF NOT EXISTS active_items (\n",
    "    bib_record_num BIGINT,\n",
    "    item_record_num BIGINT,\n",
    "    volume_record_num BIGINT,\n",
    "    volume_statement TEXT,\n",
    "    items_display_order INTEGER\n",
    ");\n",
    "\n",
    "with active_items_data as (\n",
    "  -- \"active items\"\n",
    "  -- --------------\n",
    "  -- This will produce a list of items meeting the following criteria:\n",
    "  -- * item status is one of the following codes:\n",
    "  --   ('-', '!', 'b', 'p', '(', '@', ')', '_', '=', '+', 't')\n",
    "  -- * if the item has a due date, then it must be less than 60 days overdue:\n",
    "  --   coalesce( (julianday(date('now')) - julianday(item.due_date) > 60.0 ), FALSE)\n",
    "  select\n",
    "    item.bib_record_num,\n",
    "    item.item_record_num,\n",
    "    v.volume_record_num,\n",
    "    v.volume_statement,\n",
    "    v.items_display_order\n",
    "  from\n",
    "    item\n",
    "    left outer join volume_record_item_record_link as v on v.item_record_num = item.item_record_num -- we need to consider volume information for volume-level holds\n",
    "    join record_metadata as r on (\n",
    "      r.record_type_code = 'b'\n",
    "      and r.record_num = item.bib_record_num\n",
    "      and r.campus_code = ''\n",
    "    ) -- considers only items belonging to us (no virtual items)\n",
    "  where\n",
    "    -- * item status is one of the following codes:\n",
    "    --   ('-', '!', 'b', 'p', '(', '@', ')', '_', '=', '+', 't')\n",
    "    item.item_status_code in (\n",
    "      '-',\n",
    "      '!',\n",
    "      'b',\n",
    "      'p',\n",
    "      '(',\n",
    "      '@',\n",
    "      ')',\n",
    "      '_',\n",
    "      '=',\n",
    "      '+',\n",
    "      't'\n",
    "    ) -- * if the item has a due date, then it must be less than 60 days overdue:\n",
    "    --   coalesce( (julianday(date('now')) - julianday(item.due_date) > 60.0 ), FALSE)\n",
    "    and coalesce(\n",
    "      (\n",
    "        julianday(date('now')) - julianday(item.due_date) > 60.0\n",
    "      ),\n",
    "      FALSE\n",
    "    ) is FALSE\n",
    ")\n",
    "\n",
    "INSERT OR IGNORE INTO active_items (\n",
    "    bib_record_num,\n",
    "    item_record_num,\n",
    "    volume_record_num,\n",
    "    volume_statement,\n",
    "    items_display_order\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  active_items_data\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "        \n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_active_items_bib_record_num ON active_items (bib_record_num);\n",
    "CREATE INDEX idx_active_items_item_record_num ON active_items (item_record_num);\n",
    "CREATE INDEX idx_active_items_volume_record_num ON active_items (volume_record_num);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_holds\n",
    "# \n",
    "# create the tables for active items as defined here:\n",
    "# https://ilsweb.cincinnatilibrary.org/collection-analysis-docs/static_queries_holds.html#defining-active-holds\n",
    "\n",
    "\n",
    "sql = \"\"\"\\\n",
    "CREATE TABLE IF NOT EXISTS active_holds (\n",
    "    hold_id BIGINT\n",
    ");\n",
    "\n",
    "\n",
    "with active_holds_data as (\n",
    "  -- \"active holds\"\n",
    "  -- --------------\n",
    "  -- This will produce a list of holds meeting the following criteria:\n",
    "  -- * hold that is not Frozen (except for holds placed by patrons with ptype 196)\n",
    "  -- * hold with zero delay days OR the hold delay has passed (hold placed date + delay days is not a date in the future)\n",
    "  -- * hold placed by patron with one of the following ptype codes:\n",
    "  --   ( 0, 1, 2, 5, 6, 10, 11, 12, 15, 22, 30, 31, 32, 40, 41, 196 )\n",
    "  -- * hold status is \"on hold\"\n",
    "  select\n",
    "    h.hold_id\n",
    "  from\n",
    "    hold as h\n",
    "    join record_metadata as r on (\n",
    "      -- TODO figure out if maybe we could just use the `is_ill` boolean value to do this (this is still fast since it's an indexed search)\n",
    "      r.record_type_code = 'b'\n",
    "      and r.record_num = h.bib_record_num\n",
    "      and r.campus_code = ''\n",
    "    ) -- join the record metadata so that we're only concerning ourselves with titles that belong to us (to filter out ILL holds)\n",
    "  where\n",
    "    -- * hold that is not Frozen (except for holds placed by patrons with ptype 196)\n",
    "    (\n",
    "      h.is_frozen is FALSE\n",
    "      OR h.patron_ptype_code = 196\n",
    "    )\n",
    "    AND -- * hold with zero delay days OR the hold delay has passed (hold placed date + delay days is not in the future)\n",
    "    (\n",
    "      julianday(datetime('now')) - (\n",
    "        julianday(h.placed_gmt) + (h.delay_days * 1.0)\n",
    "      )\n",
    "    ) > 0\n",
    "    AND -- * hold placed by patron with one of the following ptype codes:\n",
    "    --   ( 0, 1, 2, 5, 6, 10, 11, 12, 15, 22, 30, 31, 32, 40, 41, 196 )\n",
    "    h.patron_ptype_code IN (\n",
    "      0,\n",
    "      1,\n",
    "      2,\n",
    "      5,\n",
    "      6,\n",
    "      10,\n",
    "      11,\n",
    "      12,\n",
    "      15,\n",
    "      22,\n",
    "      30,\n",
    "      31,\n",
    "      32,\n",
    "      40,\n",
    "      41,\n",
    "      196\n",
    "    )\n",
    "    AND -- * hold status is \"on hold\"\n",
    "    h.hold_status = 'on hold'\n",
    ")\n",
    "\n",
    "INSERT OR IGNORE INTO active_holds (\n",
    "    hold_id\n",
    ")\n",
    "\n",
    "select\n",
    "  *\n",
    "from\n",
    "  active_holds_data\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "        \n",
    "sql = \"\"\"\\\n",
    "CREATE INDEX idx_active_holds_holds_id ON active_holds (hold_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rsync -Pav current_collection.db plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/collection-2021-04-12.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the full text search (fts) on the \n",
    "# best_author, best_title,\n",
    "# publisher, publish_year,\n",
    "# bib_level_callnumber, indexed_subjects\n",
    "# columns using the \n",
    "\n",
    "utils_db = sqlite_utils.Database('current_collection.db')\n",
    "utils_db[\"bib\"].enable_fts([\"best_author\", \"best_title\", \"publisher\", \"publish_year\", \"bib_level_callnumber\", \"indexed_subjects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to double check if the table now has fts enabled ...\n",
    "utils_db[\"bib\"].detect_fts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold_shelf.db\n",
    "\n",
    "!pip install -U sqlite-utils\n",
    "\n",
    "import sqlite_utils\n",
    "import sqlite3\n",
    "import os\n",
    "from base64 import b64encode\n",
    "from hashlib import pbkdf2_hmac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the holds shelf database to a working copy (holds_shelf_output_db)\n",
    "\n",
    "# TODO -- make sure to delete the \"holds_shelf_output_db\" file first\n",
    "\n",
    "holds_shelf_master_db = '/home/plchuser/plch-holds-shelf/holds_table.db'\n",
    "holds_shelf_output_db = '/home/plchuser/output/jupyter/collection-analysis/holds_table_data.db'\n",
    "\n",
    "os.system(\"rm {}\".format(holds_shelf_output_db))\n",
    "os.system(\"sqlite3 {} \\\".backup '{}'\\\"\".format(holds_shelf_master_db, holds_shelf_output_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = create_engine('sqlite:///{}'.format(holds_shelf_output_db))\n",
    "\n",
    "salt = os.urandom(256)\n",
    "iterations = 100\n",
    "\n",
    "def hash_patron(patron):\n",
    "    try:\n",
    "        return b64encode(\n",
    "            pbkdf2_hmac(\n",
    "                hash_name='sha256',\n",
    "                password=str(patron).encode(),\n",
    "                salt=salt,\n",
    "                iterations=iterations\n",
    "            )\n",
    "        ).decode('utf-8')\n",
    "    except:\n",
    "        return 0    \n",
    "\n",
    "# give it the most basic test\n",
    "patron = 12345678\n",
    "print(hash_patron(patron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\\\n",
    "DROP TABLE IF EXISTS \"holds_shelf\";\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS \"holds_shelf\" (\n",
    "    `hold_id` INTEGER,\n",
    "    `local_hold_id` INTEGER,\n",
    "    `hash_row` TEXT UNIQUE PRIMARY KEY, -- so we can track changes made to the row\n",
    "    `placed_epoch` INTEGER,\n",
    "    `patron_record_hash` TEXT, -- \n",
    "    -- `patron_record_id` INTEGER,\n",
    "    -- `patron_record_num` INTEGER,\n",
    "    `record_id` INTEGER,\n",
    "    `record_type_code` TEXT,\n",
    "    `record_num` INTEGER,\n",
    "    `item_location_code` TEXT,\n",
    "    `agency_code_num` INTEGER,\n",
    "    `checkin_statistics_group_code_num` INTEGER,\n",
    "    `checkin_statistics_group_name` TEXT,\n",
    "    `s_location_code` TEXT,\n",
    "    -- `is_frozen` INTEGER,\n",
    "    -- `delay_days` INTEGER,\n",
    "    `expires_epoch` INTEGER,\n",
    "    `status` INTEGER,\n",
    "    -- `is_ir` INTEGER,\n",
    "    `pickup_location_code` TEXT,\n",
    "    -- `is_ill` INTEGER,\n",
    "    -- `note` TEXT,\n",
    "    -- `ir_pickup_location_code` TEXT,\n",
    "    -- `ir_print_name` TEXT,\n",
    "    -- `is_ir_converted_request` INTEGER,\n",
    "    -- `patron_records_display_order` INTEGER,\n",
    "    -- `records_display_order` INTEGER,\n",
    "    `is_deleted` INTEGER NOT NULL DEFAULT 0,\n",
    "    `deleted_epoch` INTEGER,\n",
    "    `modified_epoch` INTEGER\n",
    ");\n",
    "\n",
    "\n",
    "INSERT OR IGNORE INTO \"holds_shelf\" (\n",
    "    `hold_id`,\n",
    "    `local_hold_id`,\n",
    "    `hash_row`,\n",
    "    `placed_epoch`,\n",
    "    `patron_record_hash`, \n",
    "    `record_id`,\n",
    "    `record_type_code`,\n",
    "    `record_num`,\n",
    "    `item_location_code`,\n",
    "    `agency_code_num`,\n",
    "    `checkin_statistics_group_code_num`,\n",
    "    `checkin_statistics_group_name`,\n",
    "    `s_location_code`,\n",
    "    `expires_epoch`,\n",
    "    `status`,    \n",
    "    `pickup_location_code`,\n",
    "    `is_deleted`,\n",
    "    `deleted_epoch`,\n",
    "    `modified_epoch`\n",
    ")\n",
    "\n",
    "select\n",
    "  `hold_id`,\n",
    "  `local_hold_id`,\n",
    "  `hash_row`,\n",
    "  `placed_epoch`,\n",
    "  hash_patron_record(`patron_record_id`),\n",
    "  `record_id`,\n",
    "  `record_type_code`,\n",
    "  `record_num`,\n",
    "  `item_location_code`,\n",
    "  `agency_code_num`,\n",
    "  `checkin_statistics_group_code_num`,\n",
    "  `checkin_statistics_group_name`,\n",
    "  `s_location_code`,\n",
    "  `expires_epoch`,\n",
    "  `status`,    \n",
    "  `pickup_location_code`,\n",
    "  `is_deleted`,\n",
    "  `deleted_epoch`,\n",
    "  `modified_epoch`\n",
    "from\n",
    "  data\n",
    ";\n",
    "\n",
    "DROP TABLE data\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "with sqlite3.connect(holds_shelf_output_db) as con:\n",
    "    con.create_function(\"hash_patron_record\", 1, hash_patron)\n",
    "    for statement in sql.split(';'):\n",
    "        con.execute(statement)\n",
    "        \n",
    "# os.system(\"sqlite3 {} \\\"VACUUM;\\\"\".format(holds_shelf_output_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sqlite3.connect(holds_shelf_output_db) as con:\n",
    "#     con.create_function(\"hash_patron_record\", 1, hash_patron)\n",
    "#     con.execute(\"update data set patron_record_id = hash_patron_record(patron_record_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data from the holds_shelf db into the current_collection db\n",
    "os.system(\"sqlite3 holds_table_data.db \\\".schema holds_shelf\\\" | sqlite3 current_collection.db\")\n",
    "os.system(\"sqlite3 holds_table_data.db \\\"select * from holds_shelf\\\" | sqlite3 current_collection.db \\\".import /dev/stdin holds_shelf\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the metadata.json file ...\n",
    "import json\n",
    "\n",
    "description_html = \"\"\"\\\n",
    "<p>\n",
    "    The <a href=\"current_collection\"><span style=\"color:#54AC8E;\">\"current_collection\"</span></a>\n",
    "    dataset reprsents the current collection state for the Cincinnati & Hamilton County Public Library \n",
    "    as of {}\n",
    "</p>\n",
    "<p>\n",
    "    Documentation, Static Queries, and Reports can be found in the \"data source\"\n",
    "</p>\n",
    "\"\"\".format(pd.Timestamp('now').strftime('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "sql_location_code_branch_name_available_items = \"\"\"\\\n",
    "select\n",
    "  i.location_code,\n",
    "  ln.name,\n",
    "  loc.branch_code_num,\n",
    "  bn.name as branch_name,\n",
    "  count(*) as count_available_items\n",
    "from\n",
    "  item as i\n",
    "  left outer join location as loc on loc.code = i.location_code\n",
    "  left outer join location_name as ln on ln.location_id = loc.id\n",
    "  left outer join branch as br on br.code_num = loc.branch_code_num\n",
    "  left outer join branch_name as bn on bn.branch_id = br.id\n",
    "where\n",
    "  i.item_status_code = '-'\n",
    "group by\n",
    "  i.location_code,\n",
    "  ln.name,\n",
    "  loc.branch_code_num,\n",
    "  branch_name\n",
    "order by\n",
    "  loc.branch_code_num\n",
    "\n",
    "branch_code_num_names:\n",
    "select\n",
    "  br.code_num,\n",
    "  bn.name\n",
    "\n",
    "from\n",
    "  branch as br\n",
    "  join branch_name as bn on bn.branch_id = br.id\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sql_search_titles_by_multiple_subjects = \"\"\"\\\n",
    "with bib_search as (\n",
    "  select\n",
    "    bib_record_num,\n",
    "    best_title,\n",
    "    best_author,\n",
    "    publish_year,\n",
    "    indexed_subjects\n",
    "  from\n",
    "    bib\n",
    "  where\n",
    "    indexed_subjects like trim(lower('%' || :subject1 || '%'))\n",
    "    and indexed_subjects like trim(lower('%' || :subject2 || '%'))\n",
    "    and indexed_subjects like trim(lower('%' || :subject3 || '%'))\n",
    ")\n",
    "select\n",
    "  b.best_title,\n",
    "  b.best_author,\n",
    "  cast (b.publish_year as integer) as publish_year,\n",
    "  b.indexed_subjects,\n",
    "  'https://cincinnatilibrary.bibliocommons.com/item/show/' || b.bib_record_num || '170' as catalog_link\n",
    "from\n",
    "  bib_search as b\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sql_find_locations_with_available_items_given_branch_code_num = \"\"\"\\\n",
    "with location_data as (\n",
    "  select\n",
    "    i.location_code,\n",
    "    count(*) as count_available_items\n",
    "  from\n",
    "    item as i \n",
    "  where\n",
    "    i.item_status_code = '-'\n",
    "    AND i.location_code in (\n",
    "      select\n",
    "        code\n",
    "      from\n",
    "        location as loc\n",
    "      where\n",
    "        loc.branch_code_num = :branch_code_num\n",
    "    )\n",
    "  group by\n",
    "    i.location_code\n",
    ")\n",
    "select\n",
    "  location_code,\n",
    "  (\n",
    "    select\n",
    "      ln.name\n",
    "    from\n",
    "      location as loc\n",
    "      left outer join location_name as ln on ln.location_id = loc.id\n",
    "    where\n",
    "      loc.code = d.location_code\n",
    "  ) as location_name,\n",
    "  count_available_items\n",
    "  from\n",
    "    location_data as d\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sql_top_circulating_by_subject = \"\"\"\\\n",
    "select\n",
    "  bib.best_title,\n",
    "  bib.best_author,\n",
    "  cast(bib.publish_year as integer) as publish_year,\n",
    "  sum(checkout_total + renewal_total) as total_circ,\n",
    "  'https://cincinnatilibrary.bibliocommons.com/item/show/' || bib.bib_record_num || '170' as catalog_link\n",
    "from\n",
    "  bib\n",
    "  join item as i on i.bib_record_num = bib.bib_record_num\n",
    "where\n",
    "  bib.bib_record_num in (\n",
    "    select\n",
    "      r.record_num\n",
    "    from\n",
    "      phrase_entry as e\n",
    "      join record_metadata as r on r.id = e.record_id\n",
    "    where\n",
    "      e.index_tag = 'd'\n",
    "      and index_entry LIKE lower('%' || :subject || '%')\n",
    "  )\n",
    "group by\n",
    "  bib.best_title,\n",
    "  bib.best_author,\n",
    "  bib.publish_year,\n",
    "  bib.bib_record_num\n",
    "order by\n",
    "  total_circ DESC\n",
    "\"\"\"\n",
    "\n",
    "sql_item_lookup_by_barcode = \"\"\"\\\n",
    "select \n",
    "  bib.*,\n",
    "  item.* \n",
    "from\n",
    "  item\n",
    "  join bib on bib.bib_record_num = item.bib_record_num\n",
    "where\n",
    "  barcode = upper(trim(:barcode))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sql_holds_ready_for_pickup_by_month = \"\"\"\\\n",
    "-- calculate number of items and patrons with items \"ready for pickup\" on holdshelf per month\n",
    "with hold_shelf_data as (\n",
    "  select\n",
    "    date(modified_epoch, 'unixepoch', 'localtime') as date_hold_on_holdshelf,\n",
    "    date(placed_epoch, 'unixepoch', 'localtime') as date_hold_placed,\n",
    "    cast(\n",
    "      round((modified_epoch - placed_epoch) / 86400.0) as integer\n",
    "    ) as days_to_holdshelf,\n",
    "    s_location_code as item_source_location_code,\n",
    "    record_id,\n",
    "    patron_record_hash,\n",
    "    pickup_location_code\n",
    "  from\n",
    "    holds_shelf\n",
    "  where\n",
    "    modified_epoch >= CAST(strftime('%s', :start_date || '-01') AS INT)\n",
    "    and modified_epoch < CAST(\n",
    "      strftime('%s', DATE(:start_date || '-01', '+1 months')) AS INT\n",
    "    )\n",
    ")\n",
    "select\n",
    "  strftime('%Y-%m', :start_date || '-01') as month,\n",
    "  -- pickup_location_code,\n",
    "  coalesce(branch_name.name, pickup_location_code) as pickup_location,\n",
    "  round(avg(days_to_holdshelf), 2) as avg_days_to_holdshelf,\n",
    "  count(record_id) as count_items,\n",
    "  count(DISTINCT patron_record_hash) as count_distinct_patrons,\n",
    "  round(\n",
    "    (\n",
    "      count(record_id) * 1.0 / count(DISTINCT patron_record_hash) * 1.0\n",
    "    ),\n",
    "    2\n",
    "  ) as avg_items_per_patron,\n",
    "  'https://ilsweb.cincinnatilibrary.org/collection-analysis/current_collection/holds_ready_for_pickup_by_month_branch?start_date=' || :start_date || '&branch_name=' || replace(\n",
    "    coalesce(branch_name.name, pickup_location_code),\n",
    "    ' ',\n",
    "    '%20'\n",
    "  ) || '&_hide_sql=1' as holds_ready_for_pickup_by_month_branch\n",
    "from\n",
    "  hold_shelf_data\n",
    "  left outer join \"location\" on \"location\".code = hold_shelf_data.pickup_location_code\n",
    "  left outer join branch on branch.code_num = \"location\".branch_code_num\n",
    "  left outer join branch_name on branch_name.branch_id = branch.id\n",
    "group by\n",
    "  1,\n",
    "  2\n",
    "\"\"\"\n",
    "\n",
    "sql_holds_ready_for_pickup_by_month_branch = \"\"\"\\\n",
    "with hold_shelf_data as (\n",
    "  select\n",
    "    date(modified_epoch, 'unixepoch', 'localtime') as date_hold_on_holdshelf,\n",
    "    date(placed_epoch, 'unixepoch', 'localtime') as date_hold_placed,\n",
    "    cast(\n",
    "      round((modified_epoch - placed_epoch) / 86400.0) as integer\n",
    "    ) as days_to_holdshelf,\n",
    "    s_location_code as item_source_location_code,\n",
    "    item.item_format,\n",
    "    record_num,\n",
    "    patron_record_hash,\n",
    "    pickup_location_code\n",
    "  from\n",
    "    holds_shelf as hold_shelf\n",
    "    left outer join item on item.item_record_num = hold_shelf.record_num\n",
    "  where\n",
    "    modified_epoch >= CAST(strftime('%s', :start_date || '-01') AS INT)\n",
    "    and modified_epoch < CAST(\n",
    "      strftime('%s', DATE(:start_date || '-01', '+1 months')) AS INT\n",
    "    )\n",
    ")\n",
    "select\n",
    "  strftime('%Y-%m', :start_date || '-01') as month,\n",
    "  coalesce(item_format, 'Not Available / Deleted') as item_format,\n",
    "  round(avg(days_to_holdshelf), 2) as avg_days_to_holdshelf,\n",
    "  count(record_num) as count_items,\n",
    "  count(DISTINCT patron_record_hash) as count_distinct_patrons,\n",
    "  round(\n",
    "    (\n",
    "      count(record_num) * 1.0 / count(DISTINCT patron_record_hash) * 1.0\n",
    "    ),\n",
    "    2\n",
    "  ) as avg_items_per_patron,\n",
    "  'https://ilsweb.cincinnatilibrary.org/collection-analysis/current_collection/holds_ready_for_pickup_by_month_branch_items?branch_name=' || replace(:branch_name, ' ', '%20') || '&start_date=' || :start_date || '&item_format=' || replace(item_format, ' ', '%20') || '&_hide_sql=1' as holds_ready_for_pickup_by_month_branch_items\n",
    "from\n",
    "  hold_shelf_data\n",
    "where\n",
    "  hold_shelf_data.pickup_location_code in (\n",
    "    select\n",
    "      \"location\".code\n",
    "    from\n",
    "      \"location\"\n",
    "      left outer join branch on branch.code_num = \"location\".branch_code_num\n",
    "      left outer join branch_name on branch_name.branch_id = branch.id\n",
    "    where\n",
    "      branch_name.name = :branch_name\n",
    "  )\n",
    "group by\n",
    "  1,\n",
    "  2\n",
    "order by\n",
    "  item_format\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sql_holds_ready_for_pickup_by_month_branch_items = \"\"\"\\\n",
    "with hold_shelf_data as (\n",
    "  select\n",
    "    date(modified_epoch, 'unixepoch', 'localtime') as date_hold_on_holdshelf,\n",
    "    date(placed_epoch, 'unixepoch', 'localtime') as date_hold_placed,\n",
    "    cast(\n",
    "      round((modified_epoch - placed_epoch) / 86400.0) as integer\n",
    "    ) as days_to_holdshelf,\n",
    "    s_location_code as item_source_location_code,\n",
    "    item.item_format,\n",
    "    item.bib_record_num,\n",
    "    record_num as item_record_num,\n",
    "    patron_record_hash,\n",
    "    pickup_location_code\n",
    "  from\n",
    "    holds_shelf\n",
    "    left outer join item as item on item.item_record_num = holds_shelf.record_num\n",
    "  where\n",
    "    modified_epoch >= CAST(strftime('%s', :start_date || '-01') AS INT)\n",
    "    and modified_epoch < CAST(\n",
    "      strftime('%s', DATE(:start_date || '-01', '+1 months')) AS INT\n",
    "    )\n",
    ")\n",
    "select\n",
    "  strftime('%Y-%m', :start_date || '-01') as month,\n",
    "  :branch_name as branch_name,\n",
    "  --item_source_location_code,\n",
    "  (\n",
    "    select\n",
    "      --case\n",
    "      --  when branch_name.name = :branch_name then ' ' || branch_name.name\n",
    "      --  else coalesce(branch_name.name, item_source_location_code, '')\n",
    "      --end\n",
    "      coalesce(branch_name.name, item_source_location_code, '')\n",
    "    from\n",
    "      \"location\"\n",
    "      left outer join branch on branch.code_num = \"location\".branch_code_num\n",
    "      left outer join branch_name on branch_name.branch_id = branch.id\n",
    "    where\n",
    "      \"location\".code = hold_shelf_data.item_source_location_code\n",
    "    limit\n",
    "      1\n",
    "  ) as item_source_branch_name,\n",
    "  coalesce(\n",
    "    hold_shelf_data.item_format,\n",
    "    'Not Available / Deleted'\n",
    "  ) as item_format,\n",
    "  hold_shelf_data.date_hold_placed,\n",
    "  hold_shelf_data.date_hold_on_holdshelf,\n",
    "  hold_shelf_data.days_to_holdshelf,\n",
    "  hold_shelf_data.item_record_num,\n",
    "  hold_shelf_data.bib_record_num,\n",
    "  (\n",
    "    select\n",
    "      best_title\n",
    "    from\n",
    "      bib\n",
    "    where\n",
    "      bib.bib_record_num = hold_shelf_data.bib_record_num\n",
    "    limit\n",
    "      1\n",
    "  ) as best_title\n",
    "from\n",
    "  hold_shelf_data -- left outer join current_collection.bib as bib on bib.bib_record_num = hold_shelf_data.bib_record_num\n",
    "where\n",
    "  hold_shelf_data.pickup_location_code in (\n",
    "    select\n",
    "      \"location\".code\n",
    "    from\n",
    "      \"location\"\n",
    "      left outer join branch on branch.code_num = \"location\".branch_code_num\n",
    "      left outer join branch_name on branch_name.branch_id = branch.id\n",
    "    where\n",
    "      branch_name.name = :branch_name\n",
    "  )\n",
    "  and hold_shelf_data.item_format = :item_format\n",
    "order by\n",
    "  branch_name,\n",
    "  -- use the trick of placing a ' ' in front of the name to get the branch name to sort to the top\n",
    "  case\n",
    "    when item_source_branch_name = :branch_name then ' ' || item_source_branch_name\n",
    "    else item_source_branch_name\n",
    "  end,\n",
    "  days_to_holdshelf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "json_metadata = {\n",
    "    'title': 'Current Collection Data Set',\n",
    "    'source_url': 'https://ilsweb.cincinnatilibrary.org/collection-analysis-docs/',\n",
    "    'description_html': description_html,\n",
    "    'extra_css_urls': ['/static/my.css', ],\n",
    "    'databases': {\n",
    "        'current_collection': {\n",
    "            'tables': {\n",
    "                'bib': {\n",
    "                    'fts_table': 'bib_fts',\n",
    "                    'search_mode': 'raw'\n",
    "                },\n",
    "            },       \n",
    "            'queries': {\n",
    "                'location_code_branch_name_available_items': {\n",
    "                    'sql':  sql_location_code_branch_name_available_items,\n",
    "                    'title': 'location_code_branch_name_available_items'\n",
    "                },\n",
    "                'holds_ready_for_pickup_by_month': {\n",
    "                    'sql': sql_holds_ready_for_pickup_by_month,\n",
    "                    'title': 'holds_ready_for_pickup_by_month'\n",
    "                },\n",
    "                'holds_ready_for_pickup_by_month_branch': {\n",
    "                    'sql': sql_holds_ready_for_pickup_by_month_branch,\n",
    "                    'title': 'holds_ready_for_pickup_by_month_branch'\n",
    "                },\n",
    "                'holds_ready_for_pickup_by_month_branch_items': {\n",
    "                    'sql': sql_holds_ready_for_pickup_by_month_branch_items,\n",
    "                    'title': 'holds_ready_for_pickup_by_month_branch_items'\n",
    "                },\n",
    "                'search_titles_by_multiple_subjects': {\n",
    "                    'sql': sql_search_titles_by_multiple_subjects,\n",
    "                    'title': 'search_titles_by_multiple_subjects'\n",
    "                },\n",
    "                'find_locations_with_available_items_given_branch_code_num': {\n",
    "                    'sql': sql_find_locations_with_available_items_given_branch_code_num,\n",
    "                    'title': 'find_locations_with_available_items_given_branch_code_num'\n",
    "                },\n",
    "                'top_circulating_by_subject': {\n",
    "                    'sql': sql_top_circulating_by_subject,\n",
    "                    'title': 'top_circulating_by_subject'\n",
    "                },\n",
    "                'item_lookup_by_barcode': {\n",
    "                    'sql': sql_item_lookup_by_barcode,\n",
    "                    'title': 'item_lookup_by_barcode'\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('metadata.json', 'w') as f:\n",
    "    f.write(json.dumps(json_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*:\n",
    "\n",
    "Right now, I'm managing the old snapshots in a folder on the ilsaux2 server `/home/plchuser/output/collection-analysis/datasette_hosted_databases/`\n",
    "\n",
    "* * * \n",
    "\n",
    "**from the ilsaux2 server:**\n",
    "\n",
    "```bash\n",
    "rsync -Pav \\\n",
    "    plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/ \\\n",
    "    /home/plchuser/output/collection-analysis/datasette_hosted_databases/\n",
    "```\n",
    "\n",
    "* * * \n",
    "\n",
    "Next, you may have to remove the oldest snapshot to make room on the `ilsweb` server\n",
    "\n",
    "**from the ilsweb server:**\n",
    "\n",
    "for example:\n",
    "\n",
    "```bash\n",
    "rm collection-2021-03-22.db\n",
    "```\n",
    "\n",
    "* * * \n",
    "\n",
    "sync the output from this script \n",
    "\n",
    "**from the ilsaux2 server:**\n",
    "\n",
    "```bash\n",
    "rsync -Pav /home/plchuser/output/jupyter/collection-analysis/metadata.json plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/metadata.json\n",
    "```\n",
    "\n",
    "for example ...\n",
    "\n",
    "```bash\n",
    "rsync -Pav /home/plchuser/output/jupyter/collection-analysis/current_collection.db plchuser@ilsweb.cincinnatilibrary.org://home/plchuser/data/collection-analysis/collection-2021-08-23.db\n",
    "```\n",
    "\n",
    "* * *\n",
    "\n",
    "link the correct files:\n",
    "\n",
    "**from the ilsweb server:**\n",
    "\n",
    "```bash\n",
    "rm current_collection.db\n",
    "rm collection_prev.db\n",
    "```\n",
    "\n",
    "link the new files...\n",
    "\n",
    "```bash\n",
    "ln collection-2021-08-23.db current_collection.db\n",
    "ln collection-2021-08-16.db collection_prev.db\n",
    "```\n",
    "\n",
    "Edit the `metadata.yaml` file, and change the date\n",
    "\n",
    "```bash\n",
    "nano metadata.yaml\n",
    "```\n",
    "\n",
    "restart datasette (remember to reactivate the env)\n",
    "\n",
    "```text\n",
    "(venv) plchuser@ilsweb:~/data/collection-analysis$ ./start_datasette.sh\n",
    "```\n",
    "\n",
    "**Note**\n",
    "To remove a page from the htcache (if the page is stuck with an old cached version, run this command to clean it:\n",
    "\n",
    "```bash\n",
    "sudo htcacheclean -v -p/var/cache/apache2/mod_cache_disk/ \"https://ilsweb.cincinnatilibrary.org:443/collection-analysis/?\"`\n",
    "```\n",
    "\n",
    "To remove all entries, use this:\n",
    "\n",
    "```bash\n",
    "sudo htcacheclean -v -p/var/cache/apache2/mod_cache_disk/ -r -l1k\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
